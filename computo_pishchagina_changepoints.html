<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Liudmila Pishchagina">
<meta name="author" content="Guillem Rigaill">
<meta name="author" content="Vincent Runge">
<meta name="dcterms.date" content="2023-06-05">
<meta name="keywords" content="multivariate time series, multiple change point detection, dynamic programming, functional pruning, computational geometry">

<title>Geometric-Based Pruning Rules for Change Point Detection in Multiple Independent Time Series</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="computo_pishchagina_changepoints_files/libs/clipboard/clipboard.min.js"></script>
<script src="computo_pishchagina_changepoints_files/libs/quarto-html/quarto.js"></script>
<script src="computo_pishchagina_changepoints_files/libs/quarto-html/popper.min.js"></script>
<script src="computo_pishchagina_changepoints_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="computo_pishchagina_changepoints_files/libs/quarto-html/anchor.min.js"></script>
<link href="computo_pishchagina_changepoints_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="computo_pishchagina_changepoints_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="computo_pishchagina_changepoints_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="computo_pishchagina_changepoints_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="computo_pishchagina_changepoints_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="computo_pishchagina_changepoints_files/libs/quarto-contrib/pseudocode-1.0.0/pseudocode.min.js"></script>
<link href="computo_pishchagina_changepoints_files/libs/quarto-contrib/pseudocode-1.0.0/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Geometric-Based Pruning Rules for Change Point Detection in Multiple Independent Time Series</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        <a href="https://github.com/lpishchagina">Liudmila Pishchagina</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université Paris-Saclay, CNRS, Univ Evry, LaMME, France
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        Guillem Rigaill 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université Paris-Saclay, CNRS, Univ Evry, LaMME, INRAE, IPS2, France
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://johndoe.someplace.themoon.org">Vincent Runge</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université Paris-Saclay, CNRS, Univ Evry, LaMME, France
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 5, 2023</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 9, 2023</p>
      </div>
    </div>
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">multivariate time series, multiple change point detection, dynamic programming, functional pruning, computational geometry</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <p class="date">draft</p>
                  </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series. We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for small dimensions (2, 3, 4) some of them ran significantly faster than inequality-based approaches in particular when the underlying number of changes is small compared to the data length.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#sec-changesMulti" id="toc-sec-changesMulti" class="nav-link" data-scroll-target="#sec-changesMulti"><span class="header-section-number">1</span> Functional Pruning for Multiple Time Series</a>
  <ul class="collapse">
  <li><a href="#sec-model" id="toc-sec-model" class="nav-link" data-scroll-target="#sec-model"><span class="header-section-number">1.1</span> Model and Cost</a></li>
  <li><a href="#sec-UpdateRule" id="toc-sec-UpdateRule" class="nav-link" data-scroll-target="#sec-UpdateRule"><span class="header-section-number">1.2</span> Functional Pruning Dynamic Programming Algorithm</a></li>
  <li><a href="#sec-geometry" id="toc-sec-geometry" class="nav-link" data-scroll-target="#sec-geometry"><span class="header-section-number">1.3</span> Geometric Formulation of Functional Pruning</a></li>
  </ul></li>
  <li><a href="#sec-GeomFPOP" id="toc-sec-GeomFPOP" class="nav-link" data-scroll-target="#sec-GeomFPOP"><span class="header-section-number">2</span> Geometric Functional Pruning Optimal Partitioning</a>
  <ul class="collapse">
  <li><a href="#sec-principle" id="toc-sec-principle" class="nav-link" data-scroll-target="#sec-principle"><span class="header-section-number">2.1</span> General Principle of GeomFPOP</a></li>
  </ul></li>
  <li><a href="#sec-approximation" id="toc-sec-approximation" class="nav-link" data-scroll-target="#sec-approximation"><span class="header-section-number">3</span> Approximation Operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span></a>
  <ul class="collapse">
  <li><a href="#s-type-approximation" id="toc-s-type-approximation" class="nav-link" data-scroll-target="#s-type-approximation"><span class="header-section-number">3.1</span> S-type Approximation</a></li>
  <li><a href="#r-type-approximation" id="toc-r-type-approximation" class="nav-link" data-scroll-target="#r-type-approximation"><span class="header-section-number">3.2</span> R-type Approximation</a></li>
  </ul></li>
  <li><a href="#sec-study" id="toc-sec-study" class="nav-link" data-scroll-target="#sec-study"><span class="header-section-number">4</span> Simulation Study of GeomFPOP</a>
  <ul class="collapse">
  <li><a href="#sec-NC" id="toc-sec-NC" class="nav-link" data-scroll-target="#sec-NC"><span class="header-section-number">4.1</span> The Number of Change Point Candidates stored over Time</a></li>
  <li><a href="#sec-TCsmall" id="toc-sec-TCsmall" class="nav-link" data-scroll-target="#sec-TCsmall"><span class="header-section-number">4.2</span> Empirical Time Complexity of GeomFPOP</a></li>
  <li><a href="#sec-GeomFPOP_random" id="toc-sec-GeomFPOP_random" class="nav-link" data-scroll-target="#sec-GeomFPOP_random"><span class="header-section-number">4.3</span> Empirical Time Complexity of a Randomized GeomFPOP</a></li>
  <li><a href="#sec-Run_time_p" id="toc-sec-Run_time_p" class="nav-link" data-scroll-target="#sec-Run_time_p"><span class="header-section-number">4.4</span> Empirical Complexity of the Algorithm as a Function of <span class="math inline">p</span></a></li>
  <li><a href="#sec-Run_time_segment_nb" id="toc-sec-Run_time_segment_nb" class="nav-link" data-scroll-target="#sec-Run_time_segment_nb"><span class="header-section-number">4.5</span> Run Time as a Function of the Number of Segments</a></li>
  </ul></li>
  <li><a href="#acknowledgments" id="toc-acknowledgments" class="nav-link" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
  <li><a href="#supplements" id="toc-supplements" class="nav-link" data-scroll-target="#supplements"><span class="header-section-number">5</span> Supplements</a>
  <ul class="collapse">
  <li><a href="#sec-AppendixA" id="toc-sec-AppendixA" class="nav-link" data-scroll-target="#sec-AppendixA"><span class="header-section-number">5.1</span> Examples of Likelihood-Based Cost Functions</a></li>
  <li><a href="#sec-AppendixB" id="toc-sec-AppendixB" class="nav-link" data-scroll-target="#sec-AppendixB"><span class="header-section-number">5.2</span> Arrangement of Two <span class="math inline">p</span>-balls in <span class="math inline">\mathbb R^p</span></a></li>
  <li><a href="#sec-AppendixC" id="toc-sec-AppendixC" class="nav-link" data-scroll-target="#sec-AppendixC"><span class="header-section-number">5.3</span> Intersection and Inclusion Tests</a></li>
  <li><a href="#sec-AppendixD" id="toc-sec-AppendixD" class="nav-link" data-scroll-target="#sec-AppendixD"><span class="header-section-number">5.4</span> Proof of Proposition&nbsp;3</a></li>
  <li><a href="#sec-AppendixE" id="toc-sec-AppendixE" class="nav-link" data-scroll-target="#sec-AppendixE"><span class="header-section-number">5.5</span> Optimization Strategies for GeomFPOP (R-type)</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="computo_pishchagina_changepoints.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



<section id="introduction" class="level1 unnumbered">
<h1 class="unnumbered">Introduction</h1>
<p>A National Research Council report <span class="citation" data-cites="NRCreport2013">(<a href="#ref-NRCreport2013" role="doc-biblioref">Data et al. 2013</a>)</span> has identified change point detection as one of the “inferential giants” in massive data analysis. Detecting change points, either a posteriori or online, is important in areas as diverse as bioinformatics <span class="citation" data-cites="olshen2004circular Picard2005">(<a href="#ref-olshen2004circular" role="doc-biblioref">Olshen et al. 2004</a>; <a href="#ref-Picard2005" role="doc-biblioref">Picard et al. 2005</a>)</span>, econometrics <span class="citation" data-cites="bai2003computation Aue_monitoring">(<a href="#ref-bai2003computation" role="doc-biblioref">Bai and Perron 2003</a>; <a href="#ref-Aue_monitoring" role="doc-biblioref">Aue et al. 2006</a>)</span>, medicine <span class="citation" data-cites="Bosc2003 Staudacher2005ANM Malladi2013OnlineBC">(<a href="#ref-Bosc2003" role="doc-biblioref">Bosc et al. 2003</a>; <a href="#ref-Staudacher2005ANM" role="doc-biblioref">Staudacher et al. 2005</a>; <a href="#ref-Malladi2013OnlineBC" role="doc-biblioref">Malladi, Kalamangalam, and Aazhang 2013</a>)</span>, climate and oceanography <span class="citation" data-cites="Reeves2007 DucrRobitaille2003 Killick Naoki2010">(<a href="#ref-Reeves2007" role="doc-biblioref">Reeves et al. 2007</a>; <a href="#ref-DucrRobitaille2003" role="doc-biblioref">Ducré-Robitaille, Vincent, and Boulet 2003</a>; <a href="#ref-Killick" role="doc-biblioref">Killick, Fearnhead, and Eckley 2012</a>; <a href="#ref-Naoki2010" role="doc-biblioref">Naoki and Kurths 2010</a>)</span>, finance <span class="citation" data-cites="Andreou Fryzlewicz_2014">(<a href="#ref-Andreou" role="doc-biblioref">Andreou and Ghysels 2002</a>; <a href="#ref-Fryzlewicz_2014" role="doc-biblioref">Fryzlewicz 2014</a>)</span>, autonomous driving <span class="citation" data-cites="galceran2017multipolicy">(<a href="#ref-galceran2017multipolicy" role="doc-biblioref">Galceran et al. 2017</a>)</span>, entertainment <span class="citation" data-cites="Rybach Radke Davis2006">(<a href="#ref-Rybach" role="doc-biblioref">Rybach et al. 2009</a>; <a href="#ref-Radke" role="doc-biblioref">Radke et al. 2005</a>; <a href="#ref-Davis2006" role="doc-biblioref">Davis, Lee, and Rodriguez-Yam 2006</a>)</span>, computer vision <span class="citation" data-cites="ranganathan2012pliss">(<a href="#ref-ranganathan2012pliss" role="doc-biblioref">Ranganathan 2012</a>)</span> or neuroscience <span class="citation" data-cites="jewell2020fast">(<a href="#ref-jewell2020fast" role="doc-biblioref">Jewell, Fearnhead, and Witten 2019</a>)</span>. The most common and prototypical change point detection problem is that of detecting changes in mean of a univariate Gaussian signal and a large number of approaches have been proposed to perform this task (see among many others <span class="citation" data-cites="Yao Lebarbier2005 harchaoui2010multiple Frick2013 fryzlewicz2020detecting">(<a href="#ref-Yao" role="doc-biblioref">Yao 1984</a>; <a href="#ref-Lebarbier2005" role="doc-biblioref">Lebarbier 2005</a>; <a href="#ref-harchaoui2010multiple" role="doc-biblioref">Harchaoui and Lévy-Leduc 2010</a>; <a href="#ref-Frick2013" role="doc-biblioref">Frick, Munk, and Sieling 2013</a>; <a href="#ref-fryzlewicz2020detecting" role="doc-biblioref">Anastasiou and Fryzlewicz 2022</a>)</span> and the reviews <span class="citation" data-cites="truong2020selective aminikhanghahi2017survey">(<a href="#ref-truong2020selective" role="doc-biblioref">Truong, Oudre, and Vayatis 2020</a>; <a href="#ref-aminikhanghahi2017survey" role="doc-biblioref">Aminikhanghahi and Cook 2017</a>)</span>).</p>
<p><em>Penalized cost methods.</em> Some of these methods optimize a penalized cost function (see for example <span class="citation" data-cites="Lebarbier2005 Auger jackson2005algorithm Killick Rigaill2010 Maidstone">(<a href="#ref-Lebarbier2005" role="doc-biblioref">Lebarbier 2005</a>; <a href="#ref-Auger" role="doc-biblioref">Auger and Lawrence 1989</a>; <a href="#ref-jackson2005algorithm" role="doc-biblioref">Jackson et al. 2005</a>; <a href="#ref-Killick" role="doc-biblioref">Killick, Fearnhead, and Eckley 2012</a>; <a href="#ref-Rigaill2010" role="doc-biblioref">Rigaill 2010</a>; <a href="#ref-Maidstone" role="doc-biblioref">Maidstone et al. 2017</a>)</span>. These methods have good statistical guarantees <span class="citation" data-cites="Yao lavielle2000least Lebarbier2005">(<a href="#ref-Yao" role="doc-biblioref">Yao 1984</a>; <a href="#ref-lavielle2000least" role="doc-biblioref">Lavielle and Moulines 2000</a>; <a href="#ref-Lebarbier2005" role="doc-biblioref">Lebarbier 2005</a>)</span> and have shown good performances in benchmark simulation <span class="citation" data-cites="fearnhead2018detecting">(<a href="#ref-fearnhead2018detecting" role="doc-biblioref">Fearnhead, Maidstone, and Letchford 2018</a>)</span> and on many applications <span class="citation" data-cites="lai2005comparative liehrmann2021increased">(<a href="#ref-lai2005comparative" role="doc-biblioref">Lai et al. 2005</a>; <a href="#ref-liehrmann2021increased" role="doc-biblioref">Liehrmann, Rigaill, and Hocking 2021</a>)</span>. From a computational perspective, they rely on dynamic programming algorithms that are at worst quadratic in the size of the data, <span class="math inline">n</span>. However using inequality-based and functional pruning techniques <span class="citation" data-cites="Rigaill2010 Killick Maidstone">(<a href="#ref-Rigaill2010" role="doc-biblioref">Rigaill 2010</a>; <a href="#ref-Killick" role="doc-biblioref">Killick, Fearnhead, and Eckley 2012</a>; <a href="#ref-Maidstone" role="doc-biblioref">Maidstone et al. 2017</a>)</span> the average run times are typically much smaller allowing to process very large profiles (<span class="math inline">n&gt; 10^5</span>) in a matter of seconds or minutes. In detail, for one time series:</p>
<ul>
<li>if the number of change points is proportional to <span class="math inline">n</span> both PELT (inequality-based pruning) and FPOP (functional pruning) are on average linear <span class="citation" data-cites="Killick Maidstone">(<a href="#ref-Killick" role="doc-biblioref">Killick, Fearnhead, and Eckley 2012</a>; <a href="#ref-Maidstone" role="doc-biblioref">Maidstone et al. 2017</a>)</span>;</li>
<li>if the number of change points is fixed, FPOP is quasi-linear (on simulations) while PELT is quadratic <span class="citation" data-cites="Maidstone">(<a href="#ref-Maidstone" role="doc-biblioref">Maidstone et al. 2017</a>)</span>.</li>
</ul>
<p><em>Multivariate extensions.</em> In this paper we focus on the multivariate problem assuming the cost function or log-likelihood of a segment (denoted <span class="math inline">\mathcal C</span>) can be decomposed as a sum over all <span class="math inline">p</span> dimensions. Informally that is</p>
<p><span class="math display">
\mathcal C(segment) = \sum_{k = 1}^p \mathcal C(segment, \hbox{ time series } k)\,.
</span></p>
<p>In this context, the PELT algorithm can easily be extended for multiple time series. However, as for the univariate case, it will be algorithmically efficient only if the number of change points non-neglectible compare to <span class="math inline">n</span>. In this paper, we study the extension of functional pruning techniques (and more specifically FPOP) to the multivariate case.</p>
<p>At each iteration, FPOP updates the set of parameter values for which a change position <span class="math inline">\tau</span> is optimal. As soon as this set is empty the change is pruned. For univariate time series, this set is a union of intervals in <span class="math inline">\mathbb R</span>. For multi-parametric models, this set is equal to the intersection and difference of convex sets in <span class="math inline">\mathbb R^p</span> <span class="citation" data-cites="runge2020finite">(<a href="#ref-runge2020finite" role="doc-biblioref">Runge 2020</a>)</span>. It is typically non-convex, hard to update, and deciding whether it is empty or not is not straightforward.</p>
<p>In this work, we present a new algorithm, called Geometric Functional Pruning Optimal Partitioning (GeomFPOP). The idea of our method consists in approximating the sets that are updated at each iteration of FPOP using simpler geometric shapes. Their simplicity of description and simple updating allow for a quick emptiness test.</p>
<p>The paper has the following structure. In <a href="#sec-changesMulti">Section&nbsp;1</a> we introduce the penalized optimization problem for segmented multivariate time series. We then review the existing pruned dynamic programming methods for solving this problem. We define the geometric problem that occurs when using functional pruning. The new method, called GeomFPOP, is described in <a href="#sec-GeomFPOP">Section&nbsp;2</a> and based on approximating intersection and exclusion set operators. In <a href="#sec-approximation">Section&nbsp;3</a> we introduce two approximation types (sphere-like and rectangle-like) and define the approximation operators for each of them. We then compare in <a href="#sec-study">Section&nbsp;4</a> the empirical efficiency of GeomFPOP with PELT on simulated data.</p>
</section>
<section id="sec-changesMulti" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Functional Pruning for Multiple Time Series</h1>
<section id="sec-model" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="sec-model"><span class="header-section-number">1.1</span> Model and Cost</h2>
<p>We consider the problem of change point detection in multiple time series of length <span class="math inline">n</span> and dimension <span class="math inline">p</span>. Our aim is to partition time into segments, such that in each segment the parameter associated to each time series is constant. For a time series <span class="math inline">y</span> we write <span class="math inline">y = y_{1:n} = (y_1,\dots, y_n) \in(\mathbb R^p)^n</span> with <span class="math inline">y_i^k</span> the <span class="math inline">k</span>-th component of the <span class="math inline">p</span>-dimensional point <span class="math inline">y_i\in\mathbb R^p</span> in position <span class="math inline">i</span> in vector <span class="math inline">y_{1:n}</span>. We also use the notation <span class="math inline">y _{i:j} = (y_i,\dots, y_j)</span> to denote points from index <span class="math inline">i</span> to <span class="math inline">j</span>. If we assume that there are <span class="math inline">M</span> change points in a time series, this corresponds to time series splits into <span class="math inline">M+1</span> distinct segments. Each segment <span class="math inline">m \in \{1,\dots, M+1\}</span> is generated by independent random variables from a multivariate distribution with the segment-specific parameter <span class="math inline">\theta_m = (\theta_m^1,\dots, \theta_m^p) \in \mathbb R^p</span>. A segmentation with <span class="math inline">M</span> change points is defined by the vector of integers <span class="math inline">\tau =(\tau_0 = 0, \tau_1,\dots,\tau_M,\tau_{M+1}=n)</span>. Segments are given by the sets of indices <span class="math inline">\{\tau_i+1,\dots, \tau_{i+1}\}</span> with <span class="math inline">i</span> in <span class="math inline">\{0,1,\ldots,M\}</span>.</p>
<p>We define the set <span class="math inline">S_t</span> of all possible change point locations related to the segmentation of data points between positions <span class="math inline">1</span> to <span class="math inline">t</span> as</p>
<p><span class="math display">
S_t = \{\tau = (\tau_0,\tau_1,\dots,\tau_M, \tau_{M+1}) \in \mathbb N^{M+2} | 0=\tau_{0} &lt;\tau_1 &lt; \dots &lt; \tau_M &lt; \tau_{M+1}=t\}\,.
</span></p>
<p>Usually the number of changes <span class="math inline">M</span> is unknown, and has to be estimated. Many approaches to detecting change points define a cost function for segmentation using the opposite log-likelihood (times two). Here the opposite log-likelihood (times two) linked to data point <span class="math inline">y_j</span> is given by function <span class="math inline">\theta \mapsto \Omega(\theta,y_j)</span>, where <span class="math inline">\theta = (\theta^1,\dots, \theta^p) \in \mathbb R^p</span>. Over a segment from <span class="math inline">i</span> to <span class="math inline">t</span>, the parameter remains the same and the segment cost <span class="math inline">\mathcal C</span> is given by</p>
<p><span id="eq-Cy_it"><span class="math display">
\mathcal C(y_{i:t}) = \min_{\theta \in \mathbb{R}^p} \sum_{j=i}^{t}\Omega(\theta, y_j) = \min_{\theta \in \mathbb{R}^p} \sum_{j=i}^{t} \left(\sum_{k=1}^{p} \omega(\theta^k, y_j^k)\right)\,,
\tag{1}</span></span></p>
<p>with <span class="math inline">\omega</span> the atomic likelihood function associated with <span class="math inline">\Omega</span> for each univariate time series. This decomposition is made possible by the independence hypothesis between dimensions. Notice that function <span class="math inline">\omega</span> could have been dimension-dependent with a mixture of different distributions (Gauss, Poisson, negative binomial, etc.). In our study, we use the same data model for all dimensions.</p>
<p>We consider a penalized version of the cost by a penalty <span class="math inline">\beta &gt; 0</span>, as the zero penalty case would lead to segmentation with <span class="math inline">n</span> segments. Summing over all segments we end up with a penalty that is linear in the number of segments. Such choice is common in the literature <span class="citation" data-cites="yao1988estimating Killick">(<a href="#ref-yao1988estimating" role="doc-biblioref">Yao 1988</a>; <a href="#ref-Killick" role="doc-biblioref">Killick, Fearnhead, and Eckley 2012</a>)</span> although some other penalties have been proposed <span class="citation" data-cites="Zhang2007 Lebarbier2005 Verzelen2020">(<a href="#ref-Zhang2007" role="doc-biblioref">Zhang and David 2007</a>; <a href="#ref-Lebarbier2005" role="doc-biblioref">Lebarbier 2005</a>; <a href="#ref-Verzelen2020" role="doc-biblioref">Verzelen et al. 2020</a>)</span>. The optimal penalized cost associated with our segmentation problem is then defined by</p>
<p><span id="eq-Q_n"><span class="math display">
    Q_n = \min_{\tau \in S_n} \sum_{i=0}^{M} \{\mathcal C(y_{(\tau_{i}+1):\tau_{i+1}})+\beta\}\,.
\tag{2}</span></span></p>
<p>The optimal segmentation <span class="math inline">\tau</span> is obtained by the argminimum in <a href="#eq-Q_n">Equation&nbsp;2</a>.</p>
</section>
<section id="sec-UpdateRule" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-UpdateRule"><span class="header-section-number">1.2</span> Functional Pruning Dynamic Programming Algorithm</h2>
<p>The idea of the Optimal Partitioning (OP) method <span class="citation" data-cites="jackson2005algorithm">(<a href="#ref-jackson2005algorithm" role="doc-biblioref">Jackson et al. 2005</a>)</span> is to search for the last change point defining the last segment in data <span class="math inline">y_{1:t}</span> at each iteration (with <span class="math inline">Q_0 = 0</span>), which leads to the recursion:</p>
<p><span class="math display">
Q_{t} = \min_{i\in\{0,\dots,t-1\}}\Big(Q_i + \mathcal C(y_{({i+1}:t}) + \beta \Big)\,.
</span></p>
<p><em>Functional description.</em> In the FPOP method we introduce a last segment parameter <span class="math inline">\theta = (\theta^1,\dots, \theta^p)</span> in <span class="math inline">\mathbb R^p</span> and define a functional cost <span class="math inline">\theta \mapsto Q_t(\theta)</span> depending on <span class="math inline">\theta</span>, that takes the following form:</p>
<p><span class="math display">
Q_t(\theta) = \min_{\tau \in S_t} \Big( \sum_{i=0}^{M-1} \{\mathcal C(y_{(\tau_{i}+1):\tau_{i+1}})+\beta\} + \sum_{j=\tau_{M}+1}^{t}\Omega(\theta, y_j) + \beta \Big)\,.
</span></p>
<p>As explained in <span class="citation" data-cites="Maidstone">Maidstone et al. (<a href="#ref-Maidstone" role="doc-biblioref">2017</a>)</span>, we can compute the function <span class="math inline">Q_{t+1}(\cdot)</span> based only on the knowledge of <span class="math inline">Q_{t}(\cdot)</span> as for each integer <span class="math inline">t</span> from <span class="math inline">0</span> to <span class="math inline">n-1</span>. We have:</p>
<p><span id="eq-Q_tpl1"><span class="math display">
Q_{t+1}(\theta) = \min \{Q_t(\theta),m_t +\beta \} + \Omega(\theta, y_{t+1})\,,
\tag{3}</span></span></p>
<p>for all <span class="math inline">\theta \in \mathbb R^p</span>, with <span class="math inline">m_t = \min_\theta Q_t(\theta)</span> and the initialization <span class="math inline">Q_0(\theta) = 0</span>, so that <span class="math inline">Q_1(\theta) = \Omega(\theta,y_1)</span>. By looking closely at this relation, we see that each function <span class="math inline">Q_t</span> is a piece-wise continuous function consisting of at most <span class="math inline">t</span> different functions on <span class="math inline">\mathbb R^p</span>, denoted <span class="math inline">q^i_t</span>:</p>
<p><span class="math display">
Q_t(\theta) = \min_{i \in \{1,\dots,t \}} \left\{q_t^i(\theta)\right\}\,,   
</span></p>
<p>where the <span class="math inline">q_t^i</span> functions are given by explicit formulas:</p>
<p><span class="math display">
q_t^i(\theta) = m_{i-1} + \beta + \sum_{j = i}^{t} \Omega(\theta,y_j)\,,\quad\theta \in \mathbb R^p\,,\quad i = 1,\dots,t.  
</span></p>
<p>and</p>
<p><span id="eq-m_im1"><span class="math display">
    m_{i-1} =  \min_{\theta \in \mathbb R^p}Q_{i-1}(\theta) = \min_{j \in \{ 1,\dots,i-1\}}\left\{ \min_{\theta \in \mathbb R^p}q_{i-1}^j(\theta) \right\}.
\tag{4}</span></span></p>
<p>It is important to notice that each <span class="math inline">q_t^i</span> function is associated with the last change point <span class="math inline">i-1</span> and the last segment is given by indices from <span class="math inline">i</span> to <span class="math inline">t</span>. Consequently, the last change point at step <span class="math inline">t</span> in <span class="math inline">y_{1:t}</span> is denoted as <span class="math inline">\hat\tau_t</span> <span class="math inline">( \hat \tau_t \le t-1)</span> and is given by</p>
<p><span class="math display">
\hat\tau_t = \underset{i \in \{1,\dots,t\}}{Arg\min} \left\{ \min_{\theta \in \mathbb{R}^p} q_t^i(\theta)\right\}-1.
</span></p>
<p><em>Backtracking.</em> Knowing the values of <span class="math inline">\hat{\tau}_t</span> for all <span class="math inline">t=1, \dots, n</span>, we can always restore the optimal segmentation at time <span class="math inline">n</span> for <span class="math inline">y_{1:n}</span>. This procedure is called backtracking. The vector <span class="math inline">cp(n)</span> of ordered change points in the optimal segmentation of <span class="math inline">y_{1:n}</span> is determined recursively by the relation <span class="math inline">cp(n) = (cp(\hat \tau_n), \hat \tau_n)</span> with stopping rule <span class="math inline">cp(0)=\emptyset</span>.</p>
<p><em>Parameter space description.</em> Applying functional pruning requires a precise analysis of the recursion (<a href="#eq-Q_tpl1">3</a>) that depends on the property of the cost function <span class="math inline">\Omega</span>. In what follows we consider three choices based on a Gaussian, Poisson, and negative binomial distribution for data generation. The exact formulas of these cost functions are given in <a href="#sec-AppendixA">Section&nbsp;5.1</a>.</p>
<p>We denote the set of parameter values for which the function <span class="math inline">q^i_t(\cdot)</span> is optimal as:</p>
<p><span class="math display">
Z_t^i = \left\{ \theta \in \mathbb R^p|Q_t(\theta) = q_{t}^i(\theta) \right\}, \quad i = 1,\dots,t.
</span></p>
<p>The key idea behind functional pruning is that the <span class="math inline">Z_t^i</span> are nested (<span class="math inline">Z_{t+1}^i \subset Z_t^i</span>) thus as soon as we can prove the emptiness of one set <span class="math inline">Z_t^i</span>, we delete its associated <span class="math inline">q_t^i</span> function and do not have to consider its minimum anymore at any further iteration (proof in <a href="#sec-geometry">Section&nbsp;1.3</a>). In dimension <span class="math inline">p = 1</span> this is reasonably easy. In this case, the sets <span class="math inline">Z^i_t</span> (<span class="math inline">i=1,\dots, t</span>) are unions of intervals and an efficient functional pruning rule is possible by updating a list of these intervals for <span class="math inline">Q_t</span>. This approach is implemented in FPOP <span class="citation" data-cites="Maidstone">(<a href="#ref-Maidstone" role="doc-biblioref">Maidstone et al. 2017</a>)</span>.</p>
<p>In dimension <span class="math inline">p \ge 2</span> it is not so easy anymore to keep track of the emptiness of the sets <span class="math inline">Z^i_t</span>. We illustrate the dynamics of the <span class="math inline">Z^i_t</span> sets in <a href="#fig-Figure1">Figure&nbsp;1</a> in the bi-variate Gaussian case. Each color is associated with a set <span class="math inline">Z_t^i</span> (corresponding to a possible change at <span class="math inline">i-1</span>) for <span class="math inline">t</span> equal <span class="math inline">1</span> to <span class="math inline">5</span>. This plot shows in particular that sets <span class="math inline">Z_t^i</span> can be non-convex.</p>
<div id="fig-Figure1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%201%20Z%20sets%20over%20time%20set_seed_617.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;1: The sets <span class="math inline">Z^i_t</span> over time for the bi-variate independent Gaussian model on time series without change <span class="math inline">y = \left( (0.29, 1.93), (1.86, -0.02), (0.9, 2.51), (-1.26, 0.91), (1.22, 1.11) \right)</span>. From left to right we represent at time <span class="math inline">t=1, 2, 3, 4,</span> and <span class="math inline">5</span> the parameter space <span class="math inline">(\theta^1, \theta^2).</span> Each <span class="math inline">Z^i_t</span> is represented by a color. The change <span class="math inline">1</span> associated with quadratics <span class="math inline">2</span> is pruned at <span class="math inline">t = 3</span>. Notice that each time sequence of <span class="math inline">Z^i_t</span> with <span class="math inline">i</span> fixed is a nested sequence of sets.</figcaption>
</figure>
</div>
</section>
<section id="sec-geometry" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-geometry"><span class="header-section-number">1.3</span> Geometric Formulation of Functional Pruning</h2>
<p>To build an efficient pruning strategy for dimension <span class="math inline">p \ge 2</span> we need to test the emptiness of the sets <span class="math inline">Z^i_t</span> at each iteration. Note that to get <span class="math inline">Z_t^i</span> we need to compare the functional cost <span class="math inline">q^i_t</span> with any other functional cost <span class="math inline">q^j_{t}</span>, <span class="math inline">j=1,\dots, t,\, j\neq i</span>. This leads to the definition of the following sets.</p>
<div id="def-defS" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1 </strong></span>We define <em><span class="math inline">S</span>-type set</em> <span class="math inline">S^i_j</span> using the function <span class="math inline">\Omega</span> as</p>
<p><span class="math display">
S_j^i = \left\{ \theta \in \mathbb R^p \,|\, \sum_{u=i+1}^j \Omega(\theta, y_u) \le m_{j}-m_{i}\right\}\,,\hbox{ when } i &lt; j
</span></p>
<p>and <span class="math inline">S_i^i = \mathbb R^p</span>. We denote the set of all possible S-type sets as <span class="math inline">\mathbf S</span>.</p>
<p>To ease some of our calculations, we now introduce some additional notations. For <span class="math inline">\theta = (\theta^1,\dots,\theta^p)</span> in <span class="math inline">\mathbb R^p</span>, <span class="math inline">1 \le i &lt; j \le n</span> we define <span class="math inline">p</span> univariate functions <span class="math inline">\theta^k \mapsto s^k_{ij}(\theta^k)</span> associated to the <span class="math inline">k</span>-th time series as</p>
<p><span id="eq-setS"><span class="math display">
s^k_{ij}(\theta^k)  = \sum_{u = i+1}^{j} \omega(\theta^k,y_u^k), \quad  k = 1,\dots,p\,.
\tag{5}</span></span></p>
<p>We introduce a constant <span class="math inline">\Delta_{ij}</span> and a function <span class="math inline">\theta \mapsto s_{ij}(\theta)</span>:</p>
<p><span id="eq-setSfunc"><span class="math display">
\left\{
    \begin{aligned}
       \Delta_{ij} &amp; =  \,m_j - m_{i}\,,\\
       s_{ij}(\theta) &amp; =  \sum_{k=1}^p s^k_{ij}(\theta^k)- \Delta_{ij}\,,
    \end{aligned}
    \right.
\tag{6}</span></span></p>
<p>where <span class="math inline">m_{i}</span> and <span class="math inline">m_j</span> are defined as in <a href="#eq-m_im1">Equation&nbsp;4</a>. The sets <span class="math inline">S_j^i</span> for <span class="math inline">i &lt; j</span> are also described by relation</p>
<p><span id="eq-setS"><span class="math display">
S_j^i = s_{ij}^{-1} (-\infty,0]\,.
\tag{7}</span></span> In <a href="#fig-Figure2">Figure&nbsp;2</a> we present the level curves for three different parametric models given by <span class="math inline">s_{ij}^{-1} (\{w\})</span> with <span class="math inline">w</span> a real number. Each of these curves encloses an S-type set.</p>
</div>
<div id="fig-Figure2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%202%20Contoure%20of%20S-type%20sets%20and%20cost%20.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Three examples of the level curves of a function <span class="math inline">s_{ij}</span> for bi-variate time series <span class="math inline">\{x,y\}</span>. We use the following simulations for univariate time series : (a) <span class="math inline">x\sim \mathcal{N}(0,1)</span>, <span class="math inline">y\sim \mathcal{N}(0,1)</span>, (b) <span class="math inline">x \sim \mathcal{P}(1)</span>, <span class="math inline">y \sim \mathcal{P}(3)</span>, (c) <span class="math inline">x\sim \mathcal{NB}(0.5,1)</span>, <span class="math inline">y\sim \mathcal{NB}(0.8, 1)</span>.</figcaption>
</figure>
</div>
<p>At time <span class="math inline">t = 1,\dots, n</span> we define the following sets associated to the last change point index <span class="math inline">i-1</span>:</p>
<p>-<span class="math inline">\mathtt{past}\,\mathtt{set} \,\mathcal P^i</span> <span class="math display">
\mathcal P^i =\{S_{i}^u,\, u = 1,\dots,i-1\}\,.
</span></p>
<p>-<span class="math inline">\mathtt{future}\, \mathtt{set} \,\mathcal F^i(t)</span> <span class="math display">
\mathcal F^i(t) =\{S_{v}^i, \, v = i,\dots,t\}\,.
</span></p>
<p>We denote the cardinal of a set <span class="math inline">\mathcal{A}</span> as <span class="math inline">|\mathcal{A}|</span>. Using these two sets of sets, the <span class="math inline">Z^i_t</span> have the following description.</p>
<div id="prp-proposition_sets" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 1 </strong></span>At iteration <span class="math inline">t</span>, the functional cost <span class="math inline">Q_t(\cdot)</span> defines the subsets <span class="math inline">Z_t^i</span> (<span class="math inline">i=1,\dots, t</span>), each of them being the intersection of the sets in <span class="math inline">\mathcal{F}^i(t)</span> minus the union of the sets in <span class="math inline">\mathcal P^i</span>.</p>
<p><span id="eq-setsZ"><span class="math display">
Z_t^i = (\cap_{S\in \mathcal{F}^i(t)}S) \setminus (\cup_{S\in \mathcal{P}^i}S)\,,\quad i = 1,\dots,t.
\tag{8}</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Based on the definition of the set <span class="math inline">Z_t^i</span>, the proof is straightforward. Parameter value <span class="math inline">\theta</span> is in <span class="math inline">Z_t^i</span> if and only if <span class="math inline">q_t^i(\theta) \le q_t^u(\theta)</span> for all <span class="math inline">u \ne i</span>; these inequalities define the past set (when <span class="math inline">u &lt; i</span>) and the future set (when <span class="math inline">u&gt;i</span>). By convention we assume that, in case <span class="math inline">i = t</span>, <span class="math inline">\cap_{S\in \mathcal F^i(t)}S = \mathbb R^p</span>.</p>
</div>
<div id="cor-col1" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 </strong></span>The sequence <span class="math inline">\zeta^i = (Z_t^i)_{t\ge i}</span> is a nested sequence of sets.</p>
</div>
<p>Indeed, <span class="math inline">Z_{t+1}^i</span> is equal to <span class="math inline">Z_t^i</span> with an additional intersection in the future set. Based on <a href="#cor-col1">Corollary&nbsp;1</a>, as soon as we prove that the set <span class="math inline">Z_t^i</span>, is empty, we delete its associated <span class="math inline">q_t^i</span> function and, consequently, we can prune the change point <span class="math inline">i-1</span>. In this context, functional and inequality-based pruning have a simple geometric interpretation.</p>
<p><em>Functional pruning geometry.</em> The position <span class="math inline">i-1</span> is pruned at step <span class="math inline">t+1</span>, in <span class="math inline">Q_{t+1}(\cdot),</span> if the intersection set of <span class="math inline">\cap_{S\in \mathcal F^i(t)}S</span> is covered by the union set <span class="math inline">\cup_{S\in \mathcal P^i}S</span>.</p>
<p><em>Inequality-based pruning geometry.</em> The inequality-based pruning of PELT is equivalent to the geometric rule: position <span class="math inline">i-1</span> is pruned at step <span class="math inline">t+1</span> if the set <span class="math inline">S_t^i</span> is empty. In that case, the intersection set <span class="math inline">\cap_{S\in \mathcal F^i(t)}S</span> is empty, and therefore <span class="math inline">Z_t^i</span> is also empty using <a href="#eq-setsZ">Equation&nbsp;8</a>. This shows that if a change is pruned using inequality-based pruning it is also pruned using functional pruning. For the dimension <span class="math inline">p =1</span> this claim was theoretically proved in <span class="citation" data-cites="Maidstone">Maidstone et al. (<a href="#ref-Maidstone" role="doc-biblioref">2017</a>)</span>.</p>
<p>The construction of set <span class="math inline">Z^i_t</span> using <a href="#prp-proposition_sets">Proposition&nbsp;1</a> is illustrated in <a href="#fig-Figure3">Figure&nbsp;3</a> for a bi-variate independent Gaussian case: we have the intersection of three S-type sets and the subtraction of three S-type sets.</p>
<div id="fig-Figure3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%203%20Bilding%20Z%20with%203%20past%20and%203%20future%20disks%20set_seed_21.png" class="img-fluid figure-img" style="width:50.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;3: Examples of building a set <span class="math inline">Z^i_t</span> with <span class="math inline">\lvert\mathcal P^i\rvert = \lvert\mathcal F^i(t)\rvert = 3</span> for the Gaussian case in 2-D (<span class="math inline">\mu = 0,\sigma=1</span>). The green disks are S-type sets of the past set <span class="math inline">\mathcal P^i</span>. The blue disks are S-type sets of the future set <span class="math inline">\mathcal{F}^i(t)</span>.</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-GeomFPOP" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Geometric Functional Pruning Optimal Partitioning</h1>
<section id="sec-principle" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-principle"><span class="header-section-number">2.1</span> General Principle of GeomFPOP</h2>
<p>Rather than considering an exact representation of the <span class="math inline">Z^i_t</span>, our idea is to consider a hopefully slightly larger set that is easier to update. To be specific, for each <span class="math inline">Z^i_t</span> we introduce <span class="math inline">\tilde{Z}^i_t</span>, called <em>testing set</em>, such that <span class="math inline">Z^i_t\subset \tilde{Z}^i_t</span>. If at time <span class="math inline">t</span> <span class="math inline">\tilde{Z}^i_t</span> is empty thus is <span class="math inline">Z^i_t</span> and thus change <span class="math inline">i-1</span> can be pruned. From <a href="#prp-proposition_sets">Proposition&nbsp;1</a> we have that starting from <span class="math inline">Z = \mathbb{R}^p</span> the set <span class="math inline">Z^i_t</span> is obtained by successively applying two types of operations: intersection with an S-type set <span class="math inline">S</span> <span class="math inline">(Z\cap S)</span> or subtraction of an S-type set <span class="math inline">S</span> <span class="math inline">(Z\setminus S)</span>. Similarly, starting from <span class="math inline">\tilde{Z} = \mathbb{R}^p</span> we obtain <span class="math inline">\tilde{Z}^i_t</span> by successively applying approximation of these intersection and subtraction operations. Intuitively, the complexity of the resulting algorithm is a combination of the efficiency of the pruning and the easiness of updating the testing set.</p>
<p><em>A Generic Formulation of GeomFPOP.</em> In what follows we will generically describe GeomFPOP, that is, without specifying the precise structure of the testing set <span class="math inline">\tilde{Z}^i_t</span>. We call <span class="math inline">\widetilde{\mathbf{Z}}</span> the set of all possible <span class="math inline">\tilde{Z}^i_t</span> and assume the existence of two operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span>. We have the following assumptions for these operators.</p>
<div id="def-assumption1" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2 </strong></span>The two operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span> are such that:</p>
<ol type="1">
<li>the left input is a <span class="math inline">\tilde{Z}</span>-type set (that is an element of <span class="math inline">\widetilde{\mathbf{Z}}</span>);</li>
<li>the right input is a <span class="math inline">S</span>-type set;</li>
<li>the output is a <span class="math inline">\tilde{Z}</span>-type set;</li>
<li><span class="math inline">\tilde{Z} \cap S \subset \tilde{Z} \cap_{\tilde{Z}} S</span> and <span class="math inline">\tilde{Z} \setminus S \subset \tilde{Z} \setminus_{\tilde{Z}} S</span>.</li>
</ol>
</div>
<p>We give a proper description of two types of testing sets and their approximation operators in <a href="#sec-approximation">Section&nbsp;3</a>.</p>
<p>At each iteration <span class="math inline">t</span> GeomFPOP will construct <span class="math inline">\tilde{Z}^i_{t+1}</span> from <span class="math inline">\tilde{Z}^i_{t}</span>, <span class="math inline">\mathcal P^i</span> and, <span class="math inline">\mathcal F^i(t)</span> iteratively using the two operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span>. To be specific, we define <span class="math inline">S_j^F</span> the j-th element of <span class="math inline">\mathcal F^i(t)</span> and <span class="math inline">S_P^j</span> the j-th element of <span class="math inline">\mathcal P^i</span>, we use the following iteration:</p>
<p><span class="math display">
\left\{
      \begin{aligned}
       A_{0} =\tilde{Z}^i_{t} \,, &amp; \quad A_j = A_{j-1}\,\cap_{\tilde{Z}}\, S_j^F\,, &amp; j = 1,\dots , |\mathcal{F}^i(t)|\,,\\
        B_{0} =A_{|\mathcal{F}^i(t)|}\,, &amp; \quad B_j = B_{j-1}\,\setminus_{\tilde{Z}} \, S_P^j\,, &amp; j = 1,\dots , |\mathcal{P}^i| \,,\\
    \end{aligned}  
\right.
</span></p>
<p>and define <span class="math inline">\tilde{Z}^i_{t+1} = B_{|\mathcal P^i|}.</span> Using the fourth property of <a href="#def-assumption1">Definition&nbsp;2</a> and <a href="#prp-proposition_sets">Proposition&nbsp;1</a>, we get that at any time of the algorithm <span class="math inline">\tilde{Z}^i_t</span> contains <span class="math inline">{Z}^i_t.</span></p>
<p>The pseudo-code of this procedure is described in Algorithm 1. The <span class="math inline">\mathtt{select}(\mathcal{A})</span> step in Algorithm 1, where <span class="math inline">\mathcal{A} \subset \mathbf S</span>, returns a subset of <span class="math inline">\mathcal{A}</span> in <span class="math inline">\mathbf S</span>. By default, <span class="math inline">\mathtt{select}(\mathcal{A}) := \mathcal{A}</span>.</p>
<div class="pseudocode-container" data-alg-title="Algorithm" data-pseudocode-index="1">
<div class="pseudocode">
\begin{algorithm} \caption{Geometric update rule of $\tilde{Z}^i_t$} \begin{algorithmic} \Procedure{updateZone}{$\tilde{Z}_{t-1}^i, \mathcal{P}^i, \mathcal{F}^i(t), i, t$} \State $\tilde{Z}_t^i \gets \tilde{Z}_{t-1}^i$ \For{$S \in \mathtt{select} (\mathcal{F}^i(t))$} \State $\tilde{Z}_t^i \gets \tilde{Z}_t^i\cap_{\tilde{Z}} S$ \EndFor \For{$S \in \mathtt{select} (\mathcal{P}^i)$} \State $\tilde{Z}_t^i \gets \tilde{Z}_t^i \setminus_{\tilde{Z}} S$ \EndFor \Return $\tilde{Z}_t^i$ \EndProcedure \end{algorithmic} \end{algorithm}
</div>
</div>
<p>We denote the set of candidate change points at time <span class="math inline">t</span> as <span class="math inline">\tau_t</span>. Note that for any <span class="math inline">(i-1)\in \tau_t</span> the sum of <span class="math inline">|\mathcal P^i|</span> and <span class="math inline">|\mathcal F^i(t)|</span> is <span class="math inline">|\tau_t|</span>. With the default <span class="math inline">\mathtt{select}()</span> procedure we do <span class="math inline">\mathcal{O}(p|\tau_t|)</span> operations in Algorithm 1. By limiting the number of elements returned by <span class="math inline">\mathtt{select}()</span> we can reduce the complexity.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>For example, if the operator <span class="math inline">\mathcal{A} \mapsto \mathtt{select}(\mathcal{A})</span>, regardless of <span class="math inline">|\mathcal A|</span>, always returns a subset of constant size, then the overall complexity of GeomFPOP is at worst equal to that of PELT with <span class="math inline">\sum_{t=1}^{n}\mathcal{O}(p|\tau_t|)</span> time complexity.</p>
</div>
<p>Using this <span class="math inline">\mathtt{updateZone}()</span> procedure we can now informally describe the GeomFPOP algorithm. At each iteration the algorithm will:</p>
<ol type="1">
<li>find the minimum value for <span class="math inline">Q_t</span>, <span class="math inline">m_t</span> and the best position for last change point <span class="math inline">\hat \tau_t</span> (note that this step is standard: as in the PELT algorithm we need to minimize the cost of the last segment defined in <a href="#eq-Cy_it">Equation&nbsp;1</a>);</li>
<li>compute all sets <span class="math inline">\tilde{Z}_{t}^{i}</span> using <span class="math inline">\tilde{Z}_{t-1}^{i}</span>, <span class="math inline">\mathcal{P}^i</span>, and <span class="math inline">\mathcal{F}^i(t)</span> with the <span class="math inline">\mathtt{updateZone}()</span> procedure;</li>
<li>remove changes such that <span class="math inline">\tilde{Z}_{t}^{i}</span> is empty.</li>
</ol>
<p>To simplify the pseudo-code of GeomFPOP, we also define the following operators:</p>
<ol type="1">
<li><span class="math inline">\mathtt{bestCost\&amp;Tau}(t)</span> operator returns two values: the minimum value of <span class="math inline">Q_t</span>, <span class="math inline">m_t</span>, and the best position for last change point <span class="math inline">\hat \tau_t</span> at time <span class="math inline">t</span> (see <a href="#sec-UpdateRule">Section&nbsp;1.2</a>);</li>
<li><span class="math inline">\mathtt{getPastFutureSets}(i,t)</span> operator returns a pair of sets (<span class="math inline">\mathcal F^i(t)</span>, <span class="math inline">\mathcal P^i</span>) for change point candidate <span class="math inline">i-1</span> at time <span class="math inline">t\,</span>;</li>
<li><span class="math inline">\mathtt{backtracking}(\hat\tau, n)</span> operator returns the optimal segmentation for <span class="math inline">y_{1:n}</span>.</li>
</ol>
<p>The pseudo-code of GeomFPOP is presented in Algorithm 2.</p>
<div id="alg2">
<pre class="pseudocode"><code>
\begin{algorithm}
\caption{GeomFPOP algorithm}
\begin{algorithmic}
\Procedure{GeomFPOP}{$y, \Omega(\cdot, \cdot),\beta$}
  \State $m_0 \gets 0,\quad Q_0(\theta) \gets 0\,,\quad \tau_0 \gets \emptyset, \quad \{\tilde{Z}^{i}_{i-1}\}_{i\in \{1,\dots,n\}}\gets  \mathbb{R}^p$
  \For{$t = 1, \dots, n$}
    \State $Q_t(\theta) \gets \min \{ Q_{t-1}(\theta), m_{t-1} + \beta\} + \Omega(\theta, y_t)$
    \State $(m_t, \hat\tau_t) \gets \mathtt{bestCost\&amp;Tau}(t)$
    \For{$i-1 \in \tau_t$}
      \State $(\mathcal{P}^i, \mathcal{F}^i(t)) \gets \mathtt{getPastFutureSets}(i,t)$
      \State $\tilde{Z}_t^i \gets \mathtt{updateZone}(\tilde{Z}_{t-1}^i, \mathcal{P}^i, \mathcal{F}^i(t), i, t)$
      \If{$\tilde{Z}_t^i = \emptyset$}
        \State $\tau_t \gets \tau_t \backslash\{i-1\}$
      \EndIf
    \EndFor
    \State $\tau_t \gets (\tau_{t-1}, t-1)$
  \EndFor
  \Return $cp(n) \gets \mathtt{backtracking}(\hat\tau, n)$
\EndProcedure
\end{algorithmic}
\end{algorithm}</code></pre>
</div>
</section>
</section>
<section id="sec-approximation" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Approximation Operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span></h1>
<p>The choice of the geometric structure and the way it is constructed directly affects the computational cost of the algorithm. We consider two types of testing set <span class="math inline">\tilde{Z} \in \widetilde{\mathbf{Z}}</span>, a S-type set <span class="math inline">\tilde{S}\in \mathbf{S}</span> (see <a href="#def-defS">Definition&nbsp;1</a>) and a hyperrectangle <span class="math inline">\tilde{R}\in \mathbf{R}</span> defined below.</p>
<div id="def-Hyperrectangle" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3 </strong></span>Given two vectors in <span class="math inline">\mathbb{R}^p</span>, <span class="math inline">\tilde{l}</span> and <span class="math inline">\tilde{r}</span> we define the set <span class="math inline">\tilde{R}</span>, called <em>hyperrectangle</em>, as:</p>
<p><span class="math display">
\tilde{R} = [\tilde{l}_1,\tilde{r}_1]\times \dots \times[\tilde{l}_p,\tilde{r}_p]\,. \\
</span></p>
<p>We denote the set of all possible sets <span class="math inline">\tilde{R}</span> as <span class="math inline">\mathbf{R}</span>.</p>
</div>
<p>To update the testing sets we need to give a strict definition of the operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span> for each type of testing set. To facilitate the following discussion, we rename them. For the first type of geometric structure, we rename the testing set <span class="math inline">\tilde{Z}</span> as <span class="math inline">\tilde{S}</span>, the operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span> as <span class="math inline">\cap_{S}</span> and <span class="math inline">\setminus_{S}</span> and <span class="math inline">\tilde{Z}</span>-type approximation as S-type approximation. And, likewise, we rename the testing set <span class="math inline">\tilde{Z}</span> as <span class="math inline">\tilde{R}</span>, the operators <span class="math inline">\cap_{\tilde{Z}}</span> and <span class="math inline">\setminus_{\tilde{Z}}</span> as <span class="math inline">\cap_{R}</span> and <span class="math inline">\setminus_{R}</span> and <span class="math inline">\tilde{Z}</span>-type approximation as R-type approximation for the second type of geometric structure.</p>
<section id="s-type-approximation" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="s-type-approximation"><span class="header-section-number">3.1</span> S-type Approximation</h2>
<p>With this approach, our goal is to keep track of the fact that at time <span class="math inline">t = 1,\dots, n</span> there is a pair of changes <span class="math inline">(u_1,u_2)</span>, with <span class="math inline">u_1 &lt; i &lt; u_2\le t</span> such that <span class="math inline">S^i_{u_2}\subset S^{u_1}_{i}</span> or there is a pair of changes <span class="math inline">(v_1,v_2)</span>, with <span class="math inline">i &lt; v_1 &lt; v_2\le t</span> such that <span class="math inline">S^i_{v_1}\cap S^i_{v_2}</span> is empty. If at time <span class="math inline">t</span> at least one of these conditions is met, we can guarantee that the set <span class="math inline">\tilde{S}</span> is empty, otherwise, we propose to keep as the result of approximation the last future S-type set <span class="math inline">S^i_t</span>, because it always includes the set <span class="math inline">Z^i_t</span>. This allows us to quickly check and prove (if <span class="math inline">\tilde{S} =\emptyset</span>) the emptiness of set <span class="math inline">Z^i_t</span>.</p>
<p>We consider two generic S-type sets, <span class="math inline">S</span> and <span class="math inline">\tilde{S}</span> from <span class="math inline">\mathbf{S}</span>, described as in <a href="#def-defS">Definition&nbsp;1</a> by the functions <span class="math inline">s</span> and <span class="math inline">\tilde{s}</span>:</p>
<p><span class="math display">
s(\theta) = \sum_{k=1}^p s^k(\theta^k)- \Delta\,,\quad\quad \tilde{s}(\theta) = \sum_{k=1}^p {\tilde{s}}^{k}(\theta^k)- \tilde{\Delta}\,.
</span></p>
<div id="def-def_oper_S" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 4 </strong></span>For all <span class="math inline">S</span> and <span class="math inline">\tilde{S}</span> in <span class="math inline">\mathbf{S}</span> we define the operators <span class="math inline">\cap_{S}</span> and <span class="math inline">\setminus_{S}</span> as:</p>
<p><span class="math display">
\begin{aligned}
        &amp;\tilde{S}\, \cap_{S}\, S&amp; = \left\{
        \begin{aligned}
            &amp; \emptyset \,,  &amp; \hbox{ if }  \tilde{S}\cap S = \emptyset \,,\\
            &amp; \tilde{S}\,, &amp; \hbox{otherwise}\,.\\
        \end{aligned}
        \right.\\
         &amp;\tilde{S} \,\setminus_{S}\, S   &amp; = \left\{
        \begin{aligned}
        &amp; \emptyset \,,  &amp; \hbox{ if }  \tilde{S} \subset S\,,\\
        &amp; \tilde{S}\,, &amp; \hbox{otherwise}\,.\\
        \end{aligned}
        \right.
    \end{aligned}    
</span></p>
</div>
<p>As a consequence, we only need an easy way to detect any of these two geometric configurations: <span class="math inline">\tilde{S}\cap S</span> and <span class="math inline">\tilde{S} \subset S</span>.</p>
<p>In the Gaussian case, the S-type sets are <span class="math inline">p</span>-balls and an easy solution exists based on comparing radii (see <a href="#sec-AppendixB">Section&nbsp;5.2</a> for details). In the case of other models (as Poisson or negative binomial), intersection and inclusion tests can be performed based on a solution using separative hyperplanes and iterative algorithms for convex problems (see <a href="#sec-AppendixC">Section&nbsp;5.3</a>). We propose another type of testing set solving all types of models with the same method.</p>
</section>
<section id="r-type-approximation" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="r-type-approximation"><span class="header-section-number">3.2</span> R-type Approximation</h2>
<p>Here, we approximate the sets <span class="math inline">Z^i_t</span> by hyperrectangles <span class="math inline">\tilde{R}^i_t \in \mathbf{R}</span>. A key insight of this approximation is that given a hyperrectangle <span class="math inline">R</span> and an S-type set <span class="math inline">S</span> we can efficiently (in <span class="math inline">\mathcal{O}(p)</span> using <a href="#prp-prop_solution_rect">Proposition&nbsp;3</a>) recover the best hyperrectangle approximation of <span class="math inline">R \cup S</span> and <span class="math inline">R \setminus S.</span> Formally we define these operators as follows.</p>
<div id="def-operR" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5 </strong></span>For all <span class="math inline">R, \tilde{R} \in \mathbf{R}</span> and <span class="math inline">S\in \mathbf{S}</span> we define the operators <span class="math inline">\cap_{R}</span> and <span class="math inline">\setminus_{R}</span> as: <span class="math display">
    \begin{aligned}
     R \cap_R S = \cap_{\{\tilde{R} | R \cap S \subset \mathbf{R}\}} \tilde{R}\,,\\
      R \setminus_R S = \cap_{\{\tilde{R} | R \setminus S \subset \mathbf{R}\}} \tilde{R}\,.
\end{aligned}
</span></p>
</div>
<p>We now explain how we compute these two operators. First, we note that they can be recovered by solving a <span class="math inline">2p</span> one-dimensional optimization problems.</p>
<div id="prp-proposition" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 2 </strong></span>The <span class="math inline">k</span>-th minimum coordinates <span class="math inline">\tilde{l}_k</span> and maximum coordinates <span class="math inline">\tilde{r}_k</span> of <span class="math inline">\tilde{R} = R \cap_R S</span> (resp. <span class="math inline">\tilde{R} = R \setminus_R S</span>) is obtained as</p>
<p><span id="eq-inclusionOptim"><span class="math display">
\tilde{l}_k \hbox{ or } \tilde{r}_k =
\left\{
    \begin{aligned}
        &amp;\min_{\theta_k \in \mathbb{R}} \hbox{ or } \max_{\theta_k \in \mathbb R}  \theta_k\,,\\
        &amp; \hbox{subject to } \varepsilon s(\theta) \le 0 \,,\\
        &amp; \quad \quad \quad \quad \quad l_j \le \theta_j \le r_j\,,\quad j = 1,\dots,p \,,\\
    \end{aligned}
\right.      
\tag{9}</span></span> with <span class="math inline">\varepsilon = 1</span> (resp. <span class="math inline">\varepsilon = -1</span>).</p>
</div>
<p>To solve the previous problems (<span class="math inline">\varepsilon = 1</span> or <span class="math inline">-1</span>), we define the following characteristic points.</p>
<div id="def-points" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6 </strong></span>Let <span class="math inline">S \in \mathbf S</span>, described by function <span class="math inline">s(\theta) = \sum_{k=1}^{p} s^k(\theta^k) - \Delta</span> from the family of functions (<a href="#eq-setSfunc">6</a>), with <span class="math inline">\theta\in \mathbb R^p</span>. We define the <em>minimal point</em> <span class="math inline">\mathbf{c}\in \mathbb R^p</span> of <span class="math inline">S</span> as:</p>
<p><span id="eq-c"><span class="math display">
\mathbf{c} = \left\{\mathbf{c}^k\right\}_{k=1,\dots,p}, \quad \text { with }\quad \mathbf{c}^k =\underset{\theta^k \in \mathbb R} {Arg\min} \{ s^k(\theta^k) \}\,.
\tag{10}</span></span></p>
<p>Moreover, with <span class="math inline">R \in \mathbf R</span> defined through vectors <span class="math inline">l,r \in \mathbb R^p</span>, we define two points of <span class="math inline">R</span>, the <em>closest point</em> <span class="math inline">\mathbf{m} \in \mathbb R^p</span> and the <em>farthest point</em> <span class="math inline">\mathbf M \in \mathbb R^p</span> relative to <span class="math inline">S</span> as</p>
<p><span class="math display">
\begin{aligned}
    \mathbf{m} =\left\{\mathbf{m}^k\right\}_{k=1,\dots,p},\quad \text { with }\quad
    \mathbf{m}^k = \underset{l^k \le \theta^k \le r^k}{Arg\min}  \left\{ s^k(\theta^k)\right\},\\
    \mathbf{M} =\left\{\mathbf{M}^k\right\}_{k=1,\dots,p},\quad \text { with }\quad
    \mathbf{M}^k = \underset{l^k \le \theta^k \le r^k}{Arg\max}  \left\{s^k(\theta^k)\right\}\,.
    \end{aligned}
</span></p>
</div>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>In the Gaussian case, <span class="math inline">S</span> is a ball in <span class="math inline">\mathbb R^p</span> and</p>
<ul>
<li><span class="math inline">\mathbf{c}</span> is the center of the ball;</li>
<li><span class="math inline">\mathbf{m}</span> is the closest point to <span class="math inline">\mathbf{c}</span> inside <span class="math inline">R</span>;</li>
<li><span class="math inline">\mathbf{M}</span> is the farthest point to <span class="math inline">\mathbf{c}</span> in <span class="math inline">R</span>.</li>
</ul>
</div>
<div id="fig-Figure4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%204%20Minimal%20closest%20and%20farthest%20points.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Three examples of minimal point <span class="math inline">\mathbf{c}</span>, closest point <span class="math inline">\mathbf{m}</span> and farthest point <span class="math inline">\mathbf{M}</span> for bi-variate Gaussian case: (a) <span class="math inline">R \subset S</span>; (b) <span class="math inline">R \cap S \neq \emptyset</span>; (c) <span class="math inline">R \cap S = \emptyset</span>.</figcaption>
</figure>
</div>
<div id="prp-prop_solution_rect" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3 </strong></span>Let <span class="math inline">\tilde{R} = R \cap_{R} S</span> (resp. <span class="math inline">R\setminus_{R} S</span>), with <span class="math inline">R \in \mathbf{R}</span> and <span class="math inline">S \in \mathbf{S}</span>. We compute the boundaries <span class="math inline">(\tilde{l}, \tilde{r})</span> of <span class="math inline">\tilde{R}</span> using the following rule:</p>
<ol type="1">
<li>We define the point <span class="math inline">\tilde{\theta}\in \mathbb{R}^p</span> as the closest point <span class="math inline">\mathbf{m}</span> (resp. farthest <span class="math inline">\mathbf{M}</span>). For all <span class="math inline">k = 1,\dots p</span> we find the roots <span class="math inline">\theta^{k_1}</span> and <span class="math inline">\theta^{k_2}</span> of the one-variable <span class="math inline">(\theta^k)</span> equation</li>
</ol>
<p><span class="math display">
s^k(\theta^k)+\sum_{j\neq k} s^j(\tilde{\theta}^j) -\Delta= 0 \,.
</span></p>
<p>If the roots are real-valued we consider that <span class="math inline">\theta^{k_1} \le \theta^{k_2}</span>, otherwise we write <span class="math inline">\Big[\theta^{k_1},\theta^{k_2}\Big] = \emptyset</span>.</p>
<ol start="2" type="1">
<li>We compute the boundary values <span class="math inline">\tilde{l}^k</span> and <span class="math inline">\tilde{r}^k</span> of <span class="math inline">\tilde{R}</span> as:</li>
</ol>
<ul>
<li>For <span class="math inline">R\cap_{R} S</span> <span class="math inline">(k = 1,\dots,p)</span>:</li>
</ul>
<p><span id="eq-updateIntersection"><span class="math display">
\Big[\tilde{l}^k,\tilde{r}^k\Big] = \Big[\theta^{k_1},\theta^{k_2}\Big] \cap \Big[l^k, r^k\Big]\,.
\tag{11}</span></span></p>
<ul>
<li>For <span class="math inline">R\setminus_{R} S</span> <span class="math inline">(k = 1,\dots,p)</span>: <span class="math display">
\Big[\tilde{l}^k,\tilde{r}^k\Big] =
\left\{
\begin{aligned}
&amp; \Big[l^k, r^k\Big]  \setminus \Big[\theta^{k_1},\theta^{k_2}\Big] \,,  &amp; \hbox{if} \quad \Big[\theta^{k_1},\theta^{k_2}\Big] \not\subset \Big[l^k, r^k\Big]\,,\\
&amp; \Big[l^k, r^k\Big]\,, &amp; \hbox{otherwise}\,.\\
\end{aligned}
\right.
</span></li>
</ul>
<p>If there is a dimension <span class="math inline">k</span> for which <span class="math inline">\Big[\tilde{l}^k, \tilde{r}^k\Big]=\emptyset</span>, then the set <span class="math inline">\tilde{R}</span> is empty.</p>
</div>
<p>The proof of <a href="#prp-prop_solution_rect">Proposition&nbsp;3</a> is presented in <a href="#sec-AppendixD">Section&nbsp;5.4</a>.</p>
</section>
</section>
<section id="sec-study" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Simulation Study of GeomFPOP</h1>
<p>In this section, we study the efficiency of GeomFPOP using simulations of multivariate independent time series. For this, we implemented GeomFPOP (with S and R types) and PELT for the Multivariate Independent Gaussian Model in the R-package ‘GeomFPOP’ <a href="https://github.com/lpishchagina/GeomFPOP">https://github.com/lpishchagina/GeomFPOP</a> written in R/C++. By default, the value of penalty <span class="math inline">\beta</span> for each simulation was defined by the Schwarz Information Criterion proposed in <span class="citation" data-cites="Yao">Yao (<a href="#ref-Yao" role="doc-biblioref">1984</a>)</span> (<span class="math inline">\beta = 2p \log{n}</span>).</p>
<p><em>Overview of our simulations.</em> First, as a quality control we made sure that the output of PELT and GeomFPOP were identical on a number of simulated profiles. Second, we studied cases where the PELT approach is not efficient, that is when the data has no or few changes relative to <span class="math inline">n</span>. Indeed, it was shown in <span class="citation" data-cites="Killick">Killick, Fearnhead, and Eckley (<a href="#ref-Killick" role="doc-biblioref">2012</a>)</span> and <span class="citation" data-cites="Maidstone">Maidstone et al. (<a href="#ref-Maidstone" role="doc-biblioref">2017</a>)</span> that the run time of PELT is close to <span class="math inline">\mathcal{O}(n^2)</span> in such cases. So we considered simulations of multivariate time series without change (only one segment). By these simulations we evaluated the pruning efficiency of GeomFPOP (using S and R types) for dimension <span class="math inline">2\le p\le 10</span> (see <a href="#fig-Figure5">Figure&nbsp;5</a> in <a href="#sec-NC">Section&nbsp;4.1</a>). For small dimensions (<span class="math inline">2 \le p \le 4</span>) we also evaluated the run time of GeomFPOP and PELT and compare them (see <a href="#fig-Figure6">Figure&nbsp;6</a> in <a href="#sec-TCsmall">Section&nbsp;4.2</a>). In addition, we considered another approximation of the <span class="math inline">Z^i_t</span> where we applied our <span class="math inline">\cap_{R}</span> and <span class="math inline">\setminus_R</span> operators only for a randomly selected subset of the past and future balls. In practice, this strategy turned out to be faster computationally than the full/original GeomFPOP and PELT (see <a href="#fig-Figure7">Figure&nbsp;7</a> in <a href="#sec-GeomFPOP_random">Section&nbsp;4.3</a>). For this strategy we also generated time series of a fixed size (<span class="math inline">10^6</span> data points) and varying number of segments and evaluated how the run time vary with the number of segments for small dimensions (<span class="math inline">2 \le p \le 4</span>). Our empirical results confirmed that the GeomFPOP (R-type: <span class="math inline">\mathtt{random/random}</span>) approach is computationally comparable to PELT when the number of changes is large (see <a href="#fig-Figure9">Figure&nbsp;9</a> in <a href="#sec-Run_time_segment_nb">Section&nbsp;4.5</a>).</p>
<section id="sec-NC" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-NC"><span class="header-section-number">4.1</span> The Number of Change Point Candidates stored over Time</h2>
<p>We evaluate the functional pruning efficiency of the GeomFPOP method using simulations with <span class="math inline">10^4</span> data points (without change, i.e.&nbsp;i.i.d <span class="math inline">\mathcal{N}_p(0, I_p)</span>). For such signals, PELT typically does not pruned (e.g.&nbsp;for <span class="math inline">t=10^4</span>, <span class="math inline">p=2</span> it stores almost always <span class="math inline">t</span> candidates).</p>
<p>We report in <a href="#fig-Figure5">Figure&nbsp;5</a> the percentage of candidates that are kept by GeomFPOP as a function of <span class="math inline">n</span>, <span class="math inline">p</span> and the type of pruning (R or S). Regardless of the type of approximation and contrary to PELT, we observe that there is some pruning. However when increasing the dimension <span class="math inline">p</span>, the quality of the pruning decreases.</p>
<p>Comparing <a href="#fig-Figure5">Figure&nbsp;5</a> left and the right we see that for dimensions <span class="math inline">p=2</span> to <span class="math inline">p=5</span> R-type prunes more than the S-type, while for larger dimensions the S-type prunes more than the R-type. For example, for <span class="math inline">p = 2</span> at time <span class="math inline">t=10^4</span> by GeomFPOP (R-type) the number of candidates stored over <span class="math inline">t</span> does not exceed <span class="math inline">1\%</span> versus <span class="math inline">3\%</span> by GeomFPOP (S-type). This intuitively makes sense. One the one hand the R-type approximation of a sphere gets worst with the dimension. On the other hand with R-type approximation every new approximation is included in the previous one. For small dimensions this memory effect outweight the roughness of the approximation.</p>
<div id="fig-Figure5" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%205%20Number%20of%20candidates.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Percentage of candidate change points stored over time by GeomFPOP with R (left) or S (right) type pruning for dimension <span class="math inline">p = 2,\dots, 10</span>. We simulated 100 i.i.d Gaussian data <span class="math inline">\mathcal{N}_p(0, I_p)</span> and report the average.</figcaption>
</figure>
</div>
<p>Based on these results we expect that R-type pruning GeomFPOP will be more efficient than S-type pruning for small dimensions.</p>
</section>
<section id="sec-TCsmall" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-TCsmall"><span class="header-section-number">4.2</span> Empirical Time Complexity of GeomFPOP</h2>
<p>We studied the run time of GeomFPOP (S and R-type) and compared it to PELT for small dimensions (<span class="math inline">p=2, 3, 4</span>). Run times were limited to three minutes and were recorded for simulations (without change, i.e i.i.d <span class="math inline">\mathcal{N}_p(0, I_p)</span>). The results are presented in <a href="#fig-Figure6">Figure&nbsp;6</a>. We observe that GeomFPOP is faster than PELT only for <span class="math inline">p=2</span>. For <span class="math inline">p=3</span> run times are comparable and for <span class="math inline">p=4</span> GeomFPOP is slower. This lead us to consider a randomized version of GeomFPOP (see next subsection).</p>
<div id="fig-Figure6" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%206%20Time%20complexity%20PELT%20GeomFPOP%20small%20p.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Run time of GeomFROP (S and R types) and PELT using multivariate time series without change points. The maximum run time of the algorithms is 3 minutes. Averaged over <span class="math inline">100</span> data sets.</figcaption>
</figure>
</div>
</section>
<section id="sec-GeomFPOP_random" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-GeomFPOP_random"><span class="header-section-number">4.3</span> Empirical Time Complexity of a Randomized GeomFPOP</h2>
<p>R-type GeomFPOP is designed in such a way that at each iteration we need to consider all past and future spheres of change <span class="math inline">i</span>. In practice, it is often sufficient to consider just a few of them to get an empty set. Having this in mind, we propose a further approximation of the <span class="math inline">Z^i_t</span> where we apply our <span class="math inline">\cap_{R}</span> and <span class="math inline">\setminus_R</span> operators only for a randomly selected subset of the past and future sets. In detail, we propose to redefine the output of the <span class="math inline">\mathtt{select}()</span> function in Algorithm 1 for any sets <span class="math inline">\mathcal{P}^i</span> and <span class="math inline">\mathcal{F}^i(t)</span> as:</p>
<ul>
<li><span class="math inline">\mathtt{select}(\mathcal{P}^i)</span> returns one random set from <span class="math inline">\mathcal{P}^i</span>.</li>
<li><span class="math inline">\mathtt{select}(\mathcal{F}^i(t))</span> returns the last set <span class="math inline">S^i_t</span> and one random set from <span class="math inline">\mathcal{F}^i(t)</span>.</li>
</ul>
<p>Thus, we consider the following geometric update rule:</p>
<ul>
<li><span class="math inline">(\mathtt{random / random})</span> At time <span class="math inline">t</span> we update hyperrectangle:
<ol type="1">
<li>by only two intersection operations: one with the last S-type set <span class="math inline">S^i_t</span> from <span class="math inline">\mathcal{F}^i(t)</span>, and one with a random S-type set from <span class="math inline">\mathcal{F}^i(t)</span>;</li>
<li>by only one exclusion operation with a random S-type set from <span class="math inline">\mathcal{P}^i</span>.</li>
</ol></li>
</ul>
<p>In this approach at time <span class="math inline">t</span> we do no more than three operations to update the testing set <span class="math inline">\tilde{Z}^i_t</span> for each <span class="math inline">(i-1) \in \tau_t</span>. Even with large values of <span class="math inline">p</span>, the overall complexity of GeomFPOP should not be worse than that of PELT. We investigated other randomized strategies but this simple one was sufficient to significantly improve run times. The run time of our optimization approach and PELT in dimension (<span class="math inline">p= 2, \dots, 10, 100</span>) are presented in <a href="#fig-Figure7">Figure&nbsp;7</a>. As in <a href="#sec-TCsmall">Section&nbsp;4.2</a>, run times were limited to three minutes and were recorded for simulations of length ranging from <span class="math inline">2^{10}</span> to <span class="math inline">2^{23}</span> data points (without change, i.e i.i.d <span class="math inline">\mathcal{N}_p(0, I_p)</span>).</p>
<p>Although the <span class="math inline">\mathtt{(random/random)}</span> approach reduces the quality of pruning (see <a href="#sec-AppendixE">Section&nbsp;5.5</a>), it gives a significant gain in run time compared to PELT in small dimensions. To be specific, with a run time of five minutes GeomFPOP, on average, processes a time series with a length of about <span class="math inline">8\times 10^6</span>, <span class="math inline">10^6</span> and <span class="math inline">2,5\times 10^5</span> data points in the dimensions <span class="math inline">p=2,3</span> and <span class="math inline">4</span>, respectively. At the same time, PELT manages to process time series with a length of at most <span class="math inline">6,5\times10^4</span> data points in these dimensions.</p>
<div id="fig-Figure7" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%207%20Time%20complexity%20PELT%20GeomFPOP%20p_2_10_100.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;7: Run time of the <span class="math inline">\mathtt{(random/random)}</span> approach of { GeomFPOP} (R-type) and PELT using p-variate time series without change points (<span class="math inline">p=2,\dots, 10,100</span>). The maximum run time of the algorithms is 3 minutes. Averaged over <span class="math inline">100</span> data sets.</figcaption>
</figure>
</div>
</section>
<section id="sec-Run_time_p" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-Run_time_p"><span class="header-section-number">4.4</span> Empirical Complexity of the Algorithm as a Function of <span class="math inline">p</span></h2>
<p>We also evaluate the slope coefficient <span class="math inline">\alpha</span> of the run time curve of GeomFPOP with random sampling of the past and future candidates for all considered dimensions. In <a href="#fig-Figure8">Figure&nbsp;8</a> we can see that already for <span class="math inline">p\ge 7</span> <span class="math inline">\alpha</span> is close to <span class="math inline">2</span>.</p>
<div id="fig-Figure8" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%208%20estimation%20alpha%20p_2_10_100.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;8: Run time dependence of <span class="math inline">\mathtt{(random/random)}</span> approach of GeomFPOP (R-type) on dimension <span class="math inline">p</span>.</figcaption>
</figure>
</div>
</section>
<section id="sec-Run_time_segment_nb" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="sec-Run_time_segment_nb"><span class="header-section-number">4.5</span> Run Time as a Function of the Number of Segments</h2>
<p>For small dimensions (<span class="math inline">2\le p \le 4</span>) we also generated time series with <span class="math inline">10^6</span> data points with increasing number of segments. We have considered the following number of segments: <span class="math inline">(1,2,5) \times 10^i</span>( for <span class="math inline">i=0,\dots,3</span>) and <span class="math inline">10^4</span>. The mean was equal to <span class="math inline">1</span> for even segments, and <span class="math inline">0</span> for odd segments. In <a href="#fig-Figure9">Figure&nbsp;9</a> we can see the run time dependence of the <span class="math inline">\mathtt{(random/random)}</span> approach of GeomFPOP (R-type) and PELT on the number of segments for this type of time series. Interestingly, the run time of GeomFPOP <span class="math inline">\mathtt{(random/random)}</span> is comparable to PELT even when the number of segment is large. For smaller number of segments (as already observed) GeomFPOP <span class="math inline">\mathtt{(random/random)}</span> is an order of magnitude faster.</p>
<div id="fig-Figure9" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%209%20Time%20complexity%20Change%20dependence%20.png" class="img-fluid figure-img" style="width:90.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;9: Run time dependence of <span class="math inline">\mathtt{(random/random)}</span> approach of GeomFPOP (R-type) on the number of segments in time series with <span class="math inline">10^6</span> data points.</figcaption>
</figure>
</div>
</section>
</section>
<section id="acknowledgments" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgments</h1>
<p>We thank Paul Fearnhead for fruitful discussions.</p>
</section>
<section id="supplements" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Supplements</h1>
<section id="sec-AppendixA" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-AppendixA"><span class="header-section-number">5.1</span> Examples of Likelihood-Based Cost Functions</h2>
<p>We define a cost function for segmentation as in <a href="#eq-Cy_it">Equation&nbsp;1</a> by the function <span class="math inline">\Omega(\cdot,\cdot)</span> (the opposite log-likelihood (times two)). Below is the expression of this function linked to data point <span class="math inline">y_i = (y_i^1,\dots, y_i^p)\in \mathbb R^p</span> for three examples of Parametric Multivariate Models:</p>
<p><span id="eq-MLE"><span class="math display">      
\Omega(\theta,y_i)=
\left\{
\begin{aligned}
&amp; \sum_{k=1}^p (y_i^k -\theta^k)^2\,, &amp; \text{ if }y_i \sim \mathcal{N}_p(\theta, \sigma^2\mathbb{I}_p)\,,\\
&amp;2 \sum_{k=1}^p \left\{\theta^k-\log\left(\frac{(\theta^k)^{y^k_i}}{y^k_i!}\right)\right\}\,, &amp; \text{ if }y_i \sim \mathcal{P}(\theta)\,,\\
&amp;-2 \sum_{k=1}^p\log\left((\theta^k)^{y_i^k}(1-\theta^k)^\phi \begin{pmatrix}
y_i^k+\phi-1 \\
y_i^k
\end{pmatrix}\right)\,,&amp; \text{ if }y_i \sim \mathcal{NB}(\theta,\phi)\,.\\
\end{aligned}
\right.
\tag{12}</span></span></p>
<p>We suppose that the over-dispersion parameter <span class="math inline">\phi</span> of the Multivariate Negative Binomial distribution is known.</p>
</section>
<section id="sec-AppendixB" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-AppendixB"><span class="header-section-number">5.2</span> Arrangement of Two <span class="math inline">p</span>-balls in <span class="math inline">\mathbb R^p</span></h2>
<p>We define two <span class="math inline">p</span>-balls, <span class="math inline">S</span> and <span class="math inline">S'</span> in <span class="math inline">\mathbb R^p</span> using their centers <span class="math inline">c</span>, <span class="math inline">c' \in \mathbb R^p</span> and radius <span class="math inline">R</span>, <span class="math inline">R' \in \mathbb R^{+}</span> as</p>
<p><span class="math display">
S = \{ x \in \mathbb R^p,\lvert\lvert x - c\rvert\rvert ^2 \le R^2\}\text{ and }S' = \{ x \in \mathbb R^p,\lvert\lvert x - c'\rvert\rvert ^2 \le R'^2\},
</span></p>
<p>where <span class="math inline">\lvert\lvert x - c\rvert\rvert ^2 = \sum_{k=1}^p (x^k - c^k)^2</span>, with <span class="math inline">x = (x^1,..., x^p) \in \mathbb R^p</span>, is the Euclidean norm. The distance between centers <span class="math inline">c</span> and <span class="math inline">c'</span> is defined as <span class="math inline">d(c, c') = \sqrt{\lvert\lvert c - c' \rvert\rvert^2}</span>. We have the following simple results:</p>
<p><span class="math display">
S \cap S' = \emptyset \iff d(c,c') &gt; R + R'\,,
</span></p>
<p><span class="math display">
S \subset S' \hbox{ or } S' \subset S \iff d(c,c') \le |R-R'|\,.
</span></p>
</section>
<section id="sec-AppendixC" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sec-AppendixC"><span class="header-section-number">5.3</span> Intersection and Inclusion Tests</h2>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>For any <span class="math inline">S^i_j \in \mathbf{S}</span> its associated function <span class="math inline">s</span> can be redefine after normalization by constant <span class="math inline">j-i+1</span> as:</p>
<p><span class="math display">
s(\theta) = a(\theta) +  \langle b,\theta \rangle + c,
</span></p>
<p>with <span class="math inline">a(\cdot)</span> is some convex function depending on <span class="math inline">\theta</span>, <span class="math inline">b=\{b^k\}_{k =1,\dots, p} \in \mathbb{R}^p</span> and <span class="math inline">c \in \mathbb{R}</span>.</p>
<p>For example, in the Gaussian case, the elements have the following form:</p>
<p><span class="math display">  
\begin{aligned}
&amp; a: \theta \mapsto \theta^2\,,&amp; &amp;b^k =  2\bar Y_{i:j}^k\,,&amp;&amp;c =\bar Y^2_{i:j} - \Delta_{ij}\,,
\end{aligned}
</span></p>
<p>where <span class="math inline">\bar Y_{i:j}^k = \frac{1}{j-i+1}\sum_{u=i+1}^j y_u^k</span> and <span class="math inline">\bar Y^2_{i:j} = \frac{1}{j-i+1}\sum_{u=i+1}^j \sum_{k=1}^p (y_u^k)^2</span>.</p>
</div>
<div id="def-app:func_h" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7 </strong></span>For all <span class="math inline">\theta \in \mathbb R^p</span> and <span class="math inline">S_1, S_2 \in \mathbf{S}</span> with their associated functions, <span class="math inline">s_1</span> and <span class="math inline">s_2</span>, we define a function <span class="math inline">h_{12}</span> and a hyperplane <span class="math inline">H_{12}</span> as:</p>
<p><span class="math display">  
\begin{aligned}
&amp;h_{12}(\theta):= s_2(\theta) - s_1(\theta)\,,&amp; &amp;H_{12} := \left \{\theta \in \mathbb{R}^p | h_{12}(\theta) = 0 \right \}\,.
\end{aligned}
</span></p>
<p>We denote by <span class="math inline">H_{12}^+ := \{\theta \in \mathbb{R}^p |h_{12}(\theta)&gt; 0\}</span> and <span class="math inline">H_{12}^- := \{\theta \in \mathbb{R}^p |h_{12}(\theta)&lt; 0\}</span> the positive and negative half-spaces of <span class="math inline">H_{12}</span>, respectively. We call <span class="math inline">\mathbf{H}</span> the set of hyperplanes.</p>
</div>
<p>For all <span class="math inline">S \in \mathbf{S}</span> and <span class="math inline">H \in \mathbf{H}</span> we introduce a <span class="math inline">\mathtt{half-space}</span> operator.</p>
<div id="def-halfspace" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 8 </strong></span>The operator <span class="math inline">\mathtt{half-space}</span> is such that:</p>
<ol type="1">
<li>the left input is an S-type set <span class="math inline">S</span>;</li>
<li>the right input is a hyperplane <span class="math inline">H</span>;</li>
<li>the output is the half-spaces of <span class="math inline">H</span>, such that <span class="math inline">S</span> lies in those half-spaces.</li>
</ol>
</div>
<div id="def-append:proposition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9 </strong></span>We define the output of <span class="math inline">\mathtt{half-space}(S,H)</span> by the following rule:</p>
<ol type="1">
<li>We find two points, <span class="math inline">\theta_1, \theta_2 \in \mathbb R^p</span>, as:</li>
</ol>
<p><span class="math display">  
\left\{
\begin{aligned}
\theta_1 &amp;= &amp;Arg\min s(\theta),\\
\theta_2&amp; =&amp; \left\{
\begin{aligned}
Arg\min_{\theta \in S} h(\theta),&amp; &amp;\text{if } \theta_1 \in H^+,\\
Arg\max_{\theta \in S} h(\theta), &amp; &amp; \text{if } \theta_1 \in H^-.\\
\end{aligned}
\right.
\end{aligned}
\right.
</span></p>
<ol start="2" type="1">
<li>We have:</li>
</ol>
<p><span class="math display">  
\mathtt{half-space}(S,H) = \left\{
\begin{aligned}
\{H^+\}, &amp; &amp;\text{if } \theta_1, \theta_2  \in H^+,\\
\{H^-\}, &amp; &amp; \text{if } \theta_1, \theta_2  \in H^-,\\
\{H^+, H^-\},&amp; &amp;  \text{otherwise}.\\
\end{aligned}
\right.
</span></p>
</div>
<div id="lem-petite_lemma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 </strong></span><span class="math inline">S_1 \subset H_{12}^-\Leftrightarrow \partial S_1 \subset H_{12}^-</span>, where <span class="math inline">\partial(\cdot)</span> denote the frontier operator.</p>
</div>
<p>The proof of <a href="#lem-petite_lemma">Lemma&nbsp;1</a> follows from the convexity of <span class="math inline">S_1</span>.</p>
<div id="lem-lemma:inclusion" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2 </strong></span><span class="math inline">S_1 \subset S_2</span> (resp. <span class="math inline">S_2 \subset S_1</span>) <span class="math inline">\Leftrightarrow</span> <span class="math inline">S_1, S_2 \subset H_{12}^-</span> (resp. <span class="math inline">S_1, S_2 \subset H_{12}^+</span>).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have the hypothesis <span class="math inline">\mathcal H_0:\{ S_1 \subset S_2\}</span>, then</p>
<p><span class="math display">  
\forall \theta \in  \partial S_1 \quad \left\{
\begin{aligned}
s_1(\theta) = 0, &amp; &amp;[\text{by Definition 1.1} ] \\
s_2(\theta) \le 0, &amp; &amp; [\text{by } \mathcal{H}_0] \\
\end{aligned}
\right.
\quad \Rightarrow \theta \in H_{12}^- \quad \Rightarrow \partial S_1 \subset H_{12}^-.
</span></p>
<p>Thus, according to <a href="#lem-petite_lemma">Lemma&nbsp;1</a>, <span class="math inline">S_1 \subset H_{12}^-</span>.</p>
<p>We have now the hypothesis <span class="math inline">\mathcal H_0: \{S_1, S_2 \subset H_{12}^-\}</span>, then</p>
<p><span class="math display">  
\forall \theta \in S_1 \quad \left\{
\begin{aligned}
s_1(\theta) \le 0, &amp; &amp;[\text{by Definition 1.1} ] \\
h_{12}(\theta) &lt; 0, &amp; &amp; [\text{by } \mathcal{H}_0, \text{ Definition 1.1}] \\
\end{aligned}
\right.
\quad \Rightarrow \theta \in  S_2 \quad \Rightarrow S_1 \subset S_2.
</span></p>
<p>Similarly, it is easy to show that <span class="math inline">S_2 \subset S_1\Leftrightarrow S_1, S_2 \subset H_{12}^+</span>.</p>
</div>
<div id="lem-lemma:separation" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3 </strong></span><span class="math inline">S_1\cap S_2 = \emptyset \Leftrightarrow H_{12}</span> is a separating hyperplane of <span class="math inline">S_1</span> and <span class="math inline">S_2</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We have the hypothesis <span class="math inline">\mathcal{H}_0:\{S_1~\subset~ H_{12}^+,\, S_2~\subset~ H_{12}^-\}</span>. Thus, <span class="math inline">H_{12}</span> is a separating hyperplane of <span class="math inline">S_1</span> and <span class="math inline">S_2</span> then, according to its definition, <span class="math inline">S_1\cap S_2 = \emptyset</span>.</p>
<p>We have now the hypothesis <span class="math inline">\mathcal{H}_0:\{S_1\cap S_2 = \emptyset\}</span> then</p>
<p><span class="math display">  
\forall \theta \in S_1 \quad \left\{
\begin{aligned}
s_1(\theta) \le 0, &amp; &amp;[\text{by Definition 1.1}] \\
s_2(\theta) &gt; 0, &amp; &amp; [\text{by } \mathcal{H}_0, \text{ Definition 1.1}] \\
\end{aligned}
\right.
\quad \Rightarrow \theta \in  H_{12}^+.
</span></p>
<p><span class="math display">  
\forall \theta \in S_2 \quad \left\{
\begin{aligned}
s_1(\theta) &gt; 0, &amp; &amp;[\text{by } \mathcal{H}_0, \text{ Definition 1.1}] \\
s_2(\theta) \le 0, &amp; &amp; [\text{by Definition 1.1}] \\
\end{aligned}
\right.
\quad \Rightarrow \theta \in  H_{12}^-.
</span></p>
<p>Consequently, <span class="math inline">H_{12}</span> is a separating hyperplane of <span class="math inline">S_1</span> and <span class="math inline">S_2</span>.</p>
</div>
<div id="prp-propositionApp" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 4 </strong></span>To detect set inclusion <span class="math inline">S_1 \subset S_2</span> and emptiness of set intersection <span class="math inline">S_1 \cap S_2</span>, it is necessary:</p>
<ol type="1">
<li>build the hyperplane <span class="math inline">H_{12}</span>;</li>
<li>apply the <span class="math inline">\mathtt{half-space}</span> operator for couples <span class="math inline">(S_1,H_{12})</span> and <span class="math inline">(S_2,H_{12})</span> to know in which half-space(s) <span class="math inline">S_1</span> and <span class="math inline">S_2</span> are located;</li>
<li>check the conditions in Lemmas <a href="#lem-lemma:inclusion">2</a> and <a href="#lem-lemma:separation">3</a>.</li>
</ol>
</div>
</section>
<section id="sec-AppendixD" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-AppendixD"><span class="header-section-number">5.4</span> Proof of <a href="#prp-prop_solution_rect">Proposition&nbsp;3</a></h2>
<p>For the proof of <a href="#prp-prop_solution_rect">Proposition&nbsp;3</a> we need the following remark.</p>
<div class="remark proof">
<p><span class="proof-title"><em>Remark</em>. </span>With set <span class="math inline">S\in \mathbf{S}</span> the maximum and minimum values for each coordinate in <span class="math inline">S</span> are obtained on the axis going through minimal point <span class="math inline">\mathbf{c}</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\mathbf{c} = \{\mathbf{c}^k\}_{k=1,\dots,p}</span> is the minimal point of <span class="math inline">S</span>, defined as in <a href="#eq-c">Equation&nbsp;10</a>. In the intersection case, we consider solving the optimization problem (<a href="#eq-inclusionOptim">Equation&nbsp;9</a>) for the boundaries <span class="math inline">\tilde{l}^k</span> and <span class="math inline">\tilde{r}^k</span>, removing constraint <span class="math inline">l^k \le \theta^k \le r^k</span>. If <span class="math inline">R</span> intersects <span class="math inline">S</span>, the optimal solution <span class="math inline">\theta^k</span> belongs to the boundary of <span class="math inline">S</span> due to our simple (axis-aligned rectangular) inequality constraints and we get</p>
<p><span id="eq-KKT"><span class="math display">
s^k(\theta^k) = -\sum_{ j\neq k}s^j(\theta^j)+ \Delta\,.
\tag{13}</span></span></p>
<p>We are looking for minimum and maximum values in <span class="math inline">\theta^k</span> for this equation with constraints <span class="math inline">l^j\le \theta^j \le r^j</span> (<span class="math inline">j \ne k</span>). Using the convexity of <span class="math inline">s^k</span> and <span class="math inline">s^j</span>, we need to maximize the quantity in the right-hand side. Thus, the solution <span class="math inline">\tilde{\theta}^j</span> for each <span class="math inline">\theta^j</span> is the minimal value of <span class="math inline">\sum_{j\neq k} s^j(\theta^j)</span> under constraint <span class="math inline">l^j\le \theta^j \le r^j</span> and the result can only be <span class="math inline">l^j</span>, <span class="math inline">r^j</span> or <span class="math inline">\mathbf{c}^j</span>. This decomposition in smaller problems is made possible thanks to our problem setting with independence. Looking at all coordinates at the same time, the values for <span class="math inline">\tilde{\theta}\in \mathbb R^p</span> corresponds to the closest point <span class="math inline">\mathbf{m} =\{\mathbf{m}^k\}_{k=1,\dots,p}</span>. Having found <span class="math inline">\theta^{k_1}</span> and <span class="math inline">\theta^{k_2}</span> using <span class="math inline">\tilde{\theta}</span> the result in <a href="#eq-updateIntersection">Equation&nbsp;11</a> is obvious considering current boundaries <span class="math inline">l^k</span> and <span class="math inline">r^k</span>.\ In exclusion case, we remove from <span class="math inline">R</span> the biggest possible rectangle included into <span class="math inline">S \cap \{l^j\le \theta^j \le r^j\,,\, j \ne k\}</span>, which correspond to minimizing the right hand side of <a href="#eq-KKT">Equation&nbsp;13</a>, that is maximizing <span class="math inline">\sum_{j\neq k} s^j(\theta^j)</span> under constraint <span class="math inline">l^j\le \theta^j \le r^j</span> (<span class="math inline">j \ne k</span>). In that case, the values for <span class="math inline">\tilde{\theta}</span> correspond to the greatest value returned by <span class="math inline">\sum_{j\neq k} s^j(\theta^j)</span> on interval boundaries. With convex functions <span class="math inline">s^j</span>, it corresponds to the farthest point <span class="math inline">\mathbf{M} = \{\mathbf{M}^k\}_{k=1,\dots, p}</span>.</p>
</div>
</section>
<section id="sec-AppendixE" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="sec-AppendixE"><span class="header-section-number">5.5</span> Optimization Strategies for GeomFPOP (R-type)</h2>
<p>In GeomFPOP(R-type) at each iteration, we need to consider all past and future spheres of change <span class="math inline">i</span>. As it was said in <a href="#sec-study">Section&nbsp;4</a>, in practice it is often sufficient to consider just a few of them to get an empty set. Thus, we propose to limit the number of operations <span class="math inline">\cap_R</span> no more than two:</p>
<ul>
<li><span class="math inline">\mathtt{last.}</span> At time <span class="math inline">t</span> we update hyperrectangle by only one operation, this is an intersection with the last S-type set <span class="math inline">S^i_t</span> from <span class="math inline">\mathcal{F}^i(t)</span>.</li>
<li><span class="math inline">\mathtt{random.}</span> At time <span class="math inline">t</span> we update the hyperrectangle by only two operations. First, this is an intersection with the last S-type set <span class="math inline">S^i_t</span> from <span class="math inline">\mathcal{F}^i(t)</span>, and second, this is an intersection with other random S-type set from <span class="math inline">\mathcal{F}^i(t)</span>.</li>
</ul>
<p>The number of operations <span class="math inline">\setminus_R</span> we limit no more than one:</p>
<ul>
<li><span class="math inline">\mathtt{empty.}</span> At time <span class="math inline">t</span> we do not perform <span class="math inline">\setminus_R</span> operations.</li>
<li><span class="math inline">\mathtt{random.}</span> At time <span class="math inline">t</span> we update hyperrectangle by only one operation: exclusion with a random S-type set from <span class="math inline">\mathcal{P}^i</span>.</li>
</ul>
<p>According to these notations, the approach presented in the original GeomFPOP (R-type) has the form <span class="math inline">(\mathtt{all / all})</span>. We show the impact of introduced limits on the number of change point candidates retained over time and evaluate their run times. The results are presented in Figures <a href="#fig-StrategiesAll">10</a> and <a href="#fig-RplotTCoptStrAll">11</a>.</p>
<p>Even though the <span class="math inline">\mathtt{(random/random)}</span> approach reduces the quality of pruning in dimensions <span class="math inline">p=2,3</span> and <span class="math inline">4</span>, it gives a significant gain in the run time compared to the original GeomFPOP (R-type) and is at least comparable to the <span class="math inline">\mathtt{(last/random)}</span> approach.</p>
<div id="fig-StrategiesAll" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%2010%20Optimization%20Number%20of%20candidates.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;10: Ratio number of candidate change point over time by different optimization approaches of GeomFPOP (R-type) in dimension <span class="math inline">p = 2,3</span> and <span class="math inline">4</span>. Averaged over <span class="math inline">100</span> data sets without changes with <span class="math inline">10^4</span> data points.</figcaption>
</figure>
</div>
<div id="fig-RplotTCoptStrAll" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="images/Figure%2011%20Optimization%20Time%20complexity.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;11: Run time of different optimization approaches of GeomFPOP (R-type) using multivariate time series without change points. The maximum run time of the algorithms is 3 minutes. Averaged over <span class="math inline">100</span> data sets.</figcaption>
</figure>
</div>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-aminikhanghahi2017survey" class="csl-entry" role="listitem">
Aminikhanghahi, Samaneh, and Diane J Cook. 2017. <span>“A Survey of Methods for Time Series Change Point Detection.”</span> <em>Knowledge and Information Systems</em> 51 (2): 339–67.
</div>
<div id="ref-fryzlewicz2020detecting" class="csl-entry" role="listitem">
Anastasiou, Andreas, and Piotr Fryzlewicz. 2022. <span>“Detecting Multiple Generalized Change-Points by Isolating Single Ones.”</span> <em>Metrika</em> 85 (February). <a href="https://doi.org/10.1007/s00184-021-00821-6">https://doi.org/10.1007/s00184-021-00821-6</a>.
</div>
<div id="ref-Andreou" class="csl-entry" role="listitem">
Andreou, Elena, and Eric Ghysels. 2002. <span>“Detecting Multiple Breaks in Financial Market Volatility Dynamics.”</span> <em>Journal of Applied Econometrics</em> 17 (5): 579–600. <a href="http://www.jstor.org/stable/4129273">http://www.jstor.org/stable/4129273</a>.
</div>
<div id="ref-Aue_monitoring" class="csl-entry" role="listitem">
Aue, Alexander, Lajos Horváth, Marie Hušková, and Piotr Kokoszka. 2006. <span>“Change-Point Monitoring in Linear Models.”</span> <em>The Econometrics Journal</em> 9 (3): 373–403. <a href="http://www.jstor.org/stable/23114925">http://www.jstor.org/stable/23114925</a>.
</div>
<div id="ref-Auger" class="csl-entry" role="listitem">
Auger, Ivan E., and Charles E. Lawrence. 1989. <span>“Algorithms for the Optimal Identification of Segment Neighborhoods.”</span> <em>Bulletin of Mathematical Biology</em> 51 (1): 39–54. <a href="https://doi.org/10.1007/BF02458835">https://doi.org/10.1007/BF02458835</a>.
</div>
<div id="ref-bai2003computation" class="csl-entry" role="listitem">
Bai, Jushan, and Pierre Perron. 2003. <span>“Computation and Analysis of Multiple Structural-Change.”</span> <em>Journal of Applied Econometrics</em> 18 (January).
</div>
<div id="ref-Bosc2003" class="csl-entry" role="listitem">
Bosc, Marcel, Fabrice Heitz, Jean-Paul Armspach, Izzie Namer, Daniel Gounot, and Lucien Rumbach. 2003. <span>“Automatic Change Detection in Multimodal Serial MRI: Application to Multiple Sclerosis Lesion Evolution.”</span> <em>NeuroImage 20(2)</em>, 643–56. https://doi.org/<a href="https://doi.org/10.1016/S1053-8119(03)00406-3">https://doi.org/10.1016/S1053-8119(03)00406-3</a>.
</div>
<div id="ref-NRCreport2013" class="csl-entry" role="listitem">
Data, Committee, Committee Statistics, Board Applications, Division Sciences, and National Council. 2013. <em>Frontiers in Massive Data Analysis.</em> <em>Frontiers in Massive Data Analysis</em>. <a href="https://doi.org/10.17226/18374">https://doi.org/10.17226/18374</a>.
</div>
<div id="ref-Davis2006" class="csl-entry" role="listitem">
Davis, Richard A., Thomas C. M. Lee, and Gabriel A. Rodriguez-Yam. 2006. <span>“Structural Break Estimation for Nonstationary Time Series Models.”</span> <em>Journal of the American Statistical Association</em> 101: 223–39. <a href="https://EconPapers.repec.org/RePEc:bes:jnlasa:v:101:y:2006:p:223-239">https://EconPapers.repec.org/RePEc:bes:jnlasa:v:101:y:2006:p:223-239</a>.
</div>
<div id="ref-DucrRobitaille2003" class="csl-entry" role="listitem">
Ducré-Robitaille, Jean-François, Lucie A. Vincent, and Gilles Boulet. 2003. <span>“Comparison of Techniques for Detection of Discontinuities in Temperature Series.”</span> <em>International Journal of Climatology</em> 23.
</div>
<div id="ref-fearnhead2018detecting" class="csl-entry" role="listitem">
Fearnhead, Paul, Robert Maidstone, and Adam Letchford. 2018. <span>“Detecting Changes in Slope with an L0 Penalty.”</span> <em>Journal of Computational and Graphical Statistics</em>, 1–11.
</div>
<div id="ref-Frick2013" class="csl-entry" role="listitem">
Frick, Klaus, Axel Munk, and Hannes Sieling. 2013. <span>“Multiscale Change-Point Inference.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1301.7212">https://doi.org/10.48550/ARXIV.1301.7212</a>.
</div>
<div id="ref-Fryzlewicz_2014" class="csl-entry" role="listitem">
Fryzlewicz, Piotr. 2014. <span>“Wild Binary Segmentation for Multiple Change-Point Detection.”</span> <em>The Annals of Statistics</em> 42 (6). <a href="https://doi.org/10.1214/14-aos1245">https://doi.org/10.1214/14-aos1245</a>.
</div>
<div id="ref-galceran2017multipolicy" class="csl-entry" role="listitem">
Galceran, Enric, Alexander Cunningham, Ryan Eustice, and Edwin Olson. 2017. <span>“Multipolicy Decision-Making for Autonomous Driving via Changepoint-Based Behavior Prediction: Theory and Experiment.”</span> <em>Autonomous Robots</em> 41 (August). <a href="https://doi.org/10.1007/s10514-017-9619-z">https://doi.org/10.1007/s10514-017-9619-z</a>.
</div>
<div id="ref-harchaoui2010multiple" class="csl-entry" role="listitem">
Harchaoui, Z., and C. Lévy-Leduc. 2010. <span>“Multiple Change-Point Estimation with a Total Variation Penalty.”</span> <em>Journal of the American Statistical Association.</em> 105 (492): 1480–93. <a href="http://www.jstor.org/stable/27920180">http://www.jstor.org/stable/27920180</a>.
</div>
<div id="ref-jackson2005algorithm" class="csl-entry" role="listitem">
Jackson, Brad, Jeffrey D Scargle, David Barnes, Sundararajan Arabhi, Alina Alt, Peter Gioumousis, Elyus Gwin, Paungkaew Sangtrakulcharoen, Linda Tan, and Tun Tao Tsai. 2005. <span>“An Algorithm for Optimal Partitioning of Data on an Interval.”</span> <em>IEEE Signal Processing Letters</em> 12 (2): 105–8.
</div>
<div id="ref-jewell2020fast" class="csl-entry" role="listitem">
Jewell, Sean, Paul Fearnhead, and Daniela Witten. 2019. <span>“Testing for a Change in Mean After Changepoint Detection.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1910.04291">https://doi.org/10.48550/ARXIV.1910.04291</a>.
</div>
<div id="ref-Killick" class="csl-entry" role="listitem">
Killick, Rebecca, Paul Fearnhead, and Idris A. Eckley. 2012. <span>“Optimal Detection of Changepoints with a Linear Computational Cost.”</span> <em>Journal of the American Statistical Association</em> 107 (500): 1590–98.
</div>
<div id="ref-lai2005comparative" class="csl-entry" role="listitem">
Lai, Weil R, Mark D Johnson, Raju Kucherlapati, and Peter J Park. 2005. <span>“Comparative Analysis of Algorithms for Identifying Amplifications and Deletions in Array CGH Data.”</span> <em>Bioinformatics</em> 21 (19): 3763–70.
</div>
<div id="ref-lavielle2000least" class="csl-entry" role="listitem">
Lavielle, Marc, and Eric Moulines. 2000. <span>“Least-Squares Estimation of an Unknown Number of Shifts in a Time Series.”</span> <em>Journal of Time Series Analysis</em> 21 (1): 33–59.
</div>
<div id="ref-Lebarbier2005" class="csl-entry" role="listitem">
Lebarbier, Emilie. 2005. <span>“Detecting Multiple Change-Points in the Mean of Gaussian Process by Model Selection.”</span> <em>Signal Processing</em> 85 (April): 717–36. <a href="https://doi.org/10.1016/j.sigpro.2004.11.012">https://doi.org/10.1016/j.sigpro.2004.11.012</a>.
</div>
<div id="ref-liehrmann2021increased" class="csl-entry" role="listitem">
Liehrmann, Arnaud, Guillem Rigaill, and Toby Dylan Hocking. 2021. <span>“Increased Peak Detection Accuracy in over-Dispersed ChIP-Seq Data with Supervised Segmentation Models.”</span> <em>BMC Bioinformatics</em> 22 (1): 1–18.
</div>
<div id="ref-Maidstone" class="csl-entry" role="listitem">
Maidstone, Robert, Toby Hocking, Guillem Rigaill, and Paul Fearnhead. 2017. <span>“On Optimal Multiple Changepoint Algorithms for Large Data.”</span> <em>Statistics and Computing</em> 27 (2): 519–33.
</div>
<div id="ref-Malladi2013OnlineBC" class="csl-entry" role="listitem">
Malladi, Rakesh, Giridhar P. Kalamangalam, and Behnaam Aazhang. 2013. <span>“Online Bayesian Change Point Detection Algorithms for Segmentation of Epileptic Activity.”</span> <em>2013 Asilomar Conference on Signals, Systems and Computers</em>, 1833–37.
</div>
<div id="ref-Naoki2010" class="csl-entry" role="listitem">
Naoki, Itoh, and Juergen Kurths. 2010. <span>“Change-Point Detection of Climate Time Series by Nonparametric Method.”</span> <em>Lecture Notes in Engineering and Computer Science</em> 2186 (October).
</div>
<div id="ref-olshen2004circular" class="csl-entry" role="listitem">
Olshen, Adam, E. S. Venkatraman, Robert Lucito, and Michael Wigler. 2004. <span>“Circular Binary Segmentation for the Analysis of Array-Based DNA Copy Number Data.”</span> <em>Biostatistics (Oxford, England)</em> 5 (November): 557–72. <a href="https://doi.org/10.1093/biostatistics/kxh008">https://doi.org/10.1093/biostatistics/kxh008</a>.
</div>
<div id="ref-Picard2005" class="csl-entry" role="listitem">
Picard, Franck, Stephane Robin, Marc Lavielle, Christian Vaisse, and Jean-Jacques Daudin. 2005. <span>“A Statistical Approach for Array CGH Data Analysis.”</span> <em>BMC Bioinformatics</em> 6: np. <a href="https://doi.org/10.1186/1471-2105-6-27">https://doi.org/10.1186/1471-2105-6-27</a>.
</div>
<div id="ref-Radke" class="csl-entry" role="listitem">
Radke, R. J., S. Andra, O. Al-Kofahi, and B. Roysam. 2005. <span>“Image Change Detection Algorithms: A Systematic Survey.”</span> <em>IEEE Transactions on Image Processing</em> 14 (3): 294–307. <a href="https://doi.org/10.1109/TIP.2004.838698">https://doi.org/10.1109/TIP.2004.838698</a>.
</div>
<div id="ref-ranganathan2012pliss" class="csl-entry" role="listitem">
Ranganathan, Ananth. 2012. <span>“PLISS: Labeling Places Using Online Changepoint Detection.”</span> <em>Auton. Robots</em> 32 (4): 351–68. <a href="https://doi.org/10.1007/s10514-012-9273-4">https://doi.org/10.1007/s10514-012-9273-4</a>.
</div>
<div id="ref-Reeves2007" class="csl-entry" role="listitem">
Reeves, Jaxk, Jien Chen, Xiaolan L. Wang, Robert Lund, and Qi Qi Lu. 2007. <span>“A Review and Comparison of Changepoint Detection Techniques for Climate Data.”</span> <em>Journal of Applied Meteorology and Climatology</em> 46 (6): 900–915. <a href="https://doi.org/10.1175/JAM2493.1">https://doi.org/10.1175/JAM2493.1</a>.
</div>
<div id="ref-Rigaill2010" class="csl-entry" role="listitem">
Rigaill, Guillem. 2010. <span>“A Pruned Dynamic Programming Algorithm to Recover the Best Segmentations with <span class="math inline">1</span> to <span class="math inline">K_{max}</span> Change-Points.”</span> <a href="https://doi.org/10.48550/ARXIV.1004.0887">https://doi.org/10.48550/ARXIV.1004.0887</a>.
</div>
<div id="ref-runge2020finite" class="csl-entry" role="listitem">
Runge, Vincent. 2020. <span>“Is a Finite Intersection of Balls Covered by a Finite Union of Balls in Euclidean Spaces?”</span> <em>Journal of Optimization Theory and Applications</em> 187 (2): 431–47.
</div>
<div id="ref-Rybach" class="csl-entry" role="listitem">
Rybach, David, Christian Gollan, Ralf Schluter, and Hermann Ney. 2009. <span>“Audio Segmentation for Speech Recognition Using Segment Features.”</span> In <em>2009 IEEE International Conference on Acoustics, Speech and Signal Processing</em>, 4197–4200. <a href="https://doi.org/10.1109/ICASSP.2009.4960554">https://doi.org/10.1109/ICASSP.2009.4960554</a>.
</div>
<div id="ref-Staudacher2005ANM" class="csl-entry" role="listitem">
Staudacher, Martin, Stefan Telser, Anton Amann, Hartmann Hinterhuber, and Monika Ritsch-Marte. 2005. <span>“A New Method for Change-Point Detection Developed for on-Line Analysis of the Heart Beat Variability During Sleep.”</span> <em>Physica A-Statistical Mechanics and Its Applications</em> 349: 582–96.
</div>
<div id="ref-truong2020selective" class="csl-entry" role="listitem">
Truong, Charles, Laurent Oudre, and Nicolas Vayatis. 2020. <span>“Selective Review of Offline Change Point Detection Methods.”</span> <em>Signal Processing</em> 167: 107299.
</div>
<div id="ref-Verzelen2020" class="csl-entry" role="listitem">
Verzelen, Nicolas, Magalie Fromont, Matthieu Lerasle, and Patricia Reynaud-Bouret. 2020. <span>“Optimal Change-Point Detection and Localization.”</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.2010.11470">https://doi.org/10.48550/ARXIV.2010.11470</a>.
</div>
<div id="ref-Yao" class="csl-entry" role="listitem">
Yao, Yi-Ching. 1984. <span>“Estimation of a Noisy Discrete-Time Step Function: Bayes and Empirical Bayes Approaches.”</span> <em>The Annals of Statistics</em> 12 (4): 1434–47. <a href="https://doi.org/10.1214/aos/1176346802">https://doi.org/10.1214/aos/1176346802</a>.
</div>
<div id="ref-yao1988estimating" class="csl-entry" role="listitem">
———. 1988. <span>“Estimating the Number of Change-Points via Schwarz’ Criterion.”</span> <em>Statistics &amp; Probability Letters</em> 6 (3): 181–89. <a href="https://EconPapers.repec.org/RePEc:eee:stapro:v:6:y:1988:i:3:p:181-189">https://EconPapers.repec.org/RePEc:eee:stapro:v:6:y:1988:i:3:p:181-189</a>.
</div>
<div id="ref-Zhang2007" class="csl-entry" role="listitem">
Zhang, Nancy, and Siegmund David. 2007. <span>“A Modified Bayes Information Criterion with Applications to the Analysis of Comparative Genomic Hybridization Data.”</span> <em>Biometrics</em> 63 (April): 22–32. <a href="https://doi.org/10.1111/j.1541-0420.2006.00662.x">https://doi.org/10.1111/j.1541-0420.2006.00662.x</a>.
</div>
</div>
<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div id="quarto-reuse" class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{pishchagina2023,
  author = {Pishchagina, Liudmila and Rigaill, Guillem and Runge,
    Vincent},
  publisher = {Société Française de Statistique},
  title = {Geometric-Based {Pruning} {Rules} for {Change} {Point}
    {Detection} in {Multiple} {Independent} {Time} {Series}},
  journal = {Computo},
  date = {2023-06-05},
  url = {https://computo.sfds.asso.fr/template-computo-quarto},
  doi = {xxxx},
  issn = {2824-7795},
  langid = {en},
  abstract = {We consider the problem of detecting multiple changes in
    multiple independent time series. The search for the best
    segmentation can be expressed as a minimization problem over a given
    cost function. We focus on dynamic programming algorithms that solve
    this problem exactly. When the number of changes is proportional to
    data length, an inequality-based pruning rule encoded in the PELT
    algorithm leads to a linear time complexity. Another type of
    pruning, called functional pruning, gives a close-to-linear time
    complexity whatever the number of changes, but only for the analysis
    of univariate time series. We propose a few extensions of functional
    pruning for multiple independent time series based on the use of
    simple geometric shapes (balls and hyperrectangles). We focus on the
    Gaussian case, but some of our rules can be easily extended to the
    exponential family. In a simulation study we compare the
    computational efficiency of different geometric-based pruning rules.
    We show that for small dimensions (2, 3, 4) some of them ran
    significantly faster than inequality-based approaches in particular
    when the underlying number of changes is small compared to the data
    length.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-pishchagina2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Pishchagina, Liudmila, Guillem Rigaill, and Vincent Runge. 2023.
<span>“Geometric-Based Pruning Rules for Change Point Detection in
Multiple Independent Time Series.”</span> <em>Computo</em>, June. <a href="https://doi.org/xxxx">https://doi.org/xxxx</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Geometric-Based Pruning Rules for Change Point Detection in Multiple Independent Time Series"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Liudmila Pishchagina</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: true</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    email: liudmila.pishchagina@univ-evry.fr</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://github.com/lpishchagina</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CNRS, Univ Evry, LaMME, France</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Guillem Rigaill</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">    email: guillem.rigaill@inrae.fr</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CNRS, Univ Evry, LaMME, INRAE, IPS2, France</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Vincent Runge</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">    email: vincent.runge@univ-evry.fr</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">    url: https://johndoe.someplace.themoon.org</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CNRS, Univ Evry, LaMME, France</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2023-06-05</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> &gt;+</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co"> We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series. We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for small dimensions (2, 3, 4) some of them ran significantly faster than inequality-based approaches in particular when the underlying number of changes is small compared to the data length.</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> [multivariate time series, multiple change point detection, dynamic programming, functional pruning, computational geometry]</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "xxxx"</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co">  url: https://computo.sfds.asso.fr/template-computo-quarto</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="co">  publisher: "Société Française de Statistique"</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computorg</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> "template-computo-r"</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> true # set to false once the build is running</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> false # will be set to true once accepted</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction {.unnumbered}</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>A National Research Council report <span class="co">[</span><span class="ot">@NRCreport2013</span><span class="co">]</span> has identified change point detection as one of the "inferential giants" in massive data analysis. Detecting change points, either a posteriori or online, is important in areas as diverse as bioinformatics <span class="co">[</span><span class="ot">@olshen2004circular; @Picard2005</span><span class="co">]</span>, econometrics <span class="co">[</span><span class="ot">@bai2003computation; @Aue_monitoring</span><span class="co">]</span>, medicine <span class="co">[</span><span class="ot">@Bosc2003; @Staudacher2005ANM; @Malladi2013OnlineBC</span><span class="co">]</span>, climate and oceanography <span class="co">[</span><span class="ot">@Reeves2007; @DucrRobitaille2003; @Killick;  @Naoki2010</span><span class="co">]</span>, finance <span class="co">[</span><span class="ot">@Andreou; @Fryzlewicz_2014</span><span class="co">]</span>, autonomous driving <span class="co">[</span><span class="ot">@galceran2017multipolicy</span><span class="co">]</span>, entertainment <span class="co">[</span><span class="ot">@Rybach; @Radke; @Davis2006</span><span class="co">]</span>, computer vision <span class="co">[</span><span class="ot">@ranganathan2012pliss</span><span class="co">]</span> or neuroscience <span class="co">[</span><span class="ot">@jewell2020fast</span><span class="co">]</span>. The most common and prototypical change point detection problem is that of detecting changes in mean of a univariate Gaussian signal and a large number of approaches have been proposed to perform this task (see among many others <span class="co">[</span><span class="ot">@Yao; @Lebarbier2005;  @harchaoui2010multiple; @Frick2013; @fryzlewicz2020detecting</span><span class="co">]</span> and the reviews <span class="co">[</span><span class="ot">@truong2020selective; @aminikhanghahi2017survey</span><span class="co">]</span>).</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>*Penalized cost methods.* Some of these methods optimize a penalized cost function (see for example <span class="co">[</span><span class="ot">@Lebarbier2005; @Auger; @jackson2005algorithm; @Killick; @Rigaill2010; @Maidstone</span><span class="co">]</span>. These methods have good statistical guarantees <span class="co">[</span><span class="ot">@Yao; @lavielle2000least; @Lebarbier2005</span><span class="co">]</span> and have shown good performances in benchmark simulation <span class="co">[</span><span class="ot">@fearnhead2018detecting</span><span class="co">]</span> and on many applications <span class="co">[</span><span class="ot">@lai2005comparative; @liehrmann2021increased</span><span class="co">]</span>. From a computational perspective, they rely on dynamic programming algorithms that are at worst quadratic in the size of the data, $n$. However using inequality-based and functional pruning techniques <span class="co">[</span><span class="ot">@Rigaill2010; @Killick; @Maidstone</span><span class="co">]</span> the average run times are typically much smaller allowing to process very large profiles ($n&gt; 10^5$) in a matter of seconds or minutes. In detail, for one time series:</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>if the number of change points is proportional to $n$ both PELT (inequality-based pruning) and FPOP (functional pruning) are on average linear <span class="co">[</span><span class="ot">@Killick; @Maidstone</span><span class="co">]</span>;</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>if the number of change points is fixed, FPOP is quasi-linear (on simulations) while PELT is quadratic <span class="co">[</span><span class="ot">@Maidstone</span><span class="co">]</span>.</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>*Multivariate extensions.* In this paper we focus on the multivariate problem assuming the cost function or log-likelihood of a segment (denoted $\mathcal C$) can be decomposed as a sum over all $p$ dimensions. Informally that is</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>\mathcal C(segment) = \sum_{k = 1}^p \mathcal C(segment, \hbox{ time series } k)\,.</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>In this context, the PELT algorithm can easily be extended for multiple time series. However, as for the univariate case, it will be algorithmically efficient only if the number of change points non-neglectible compare to $n$. In this paper, we study the extension of functional pruning techniques (and more specifically FPOP) to the multivariate case.</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>At each iteration, FPOP updates the set of parameter values for which a change position $\tau$ is optimal. As soon as this set is empty the change is pruned. For univariate time series, this set is a union of intervals in $\mathbb R$. For multi-parametric models, this set is equal to the intersection and difference of convex sets in $\mathbb R^p$ <span class="co">[</span><span class="ot">@runge2020finite</span><span class="co">]</span>. It is typically non-convex, hard to update, and deciding whether it is empty or not is not straightforward.</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>In this work, we present a new algorithm, called Geometric Functional Pruning Optimal Partitioning (GeomFPOP). The idea of our method consists in approximating the sets that are updated at each iteration of FPOP using simpler geometric shapes. Their simplicity of description and simple updating allow for a quick emptiness test.</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>The paper has the following structure. In @sec-changesMulti we introduce the penalized optimization problem for segmented multivariate time series. We then review the existing pruned dynamic programming methods for solving this problem. We define the geometric problem that occurs when using functional pruning. The new method, called GeomFPOP, is described in @sec-GeomFPOP and based on approximating intersection and exclusion set operators. In @sec-approximation we introduce two approximation types (sphere-like and rectangle-like) and define the approximation operators for each of them. We then compare in @sec-study the empirical efficiency of GeomFPOP with PELT on simulated data. </span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="fu"># Functional Pruning for Multiple Time Series{#sec-changesMulti}</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model and Cost{#sec-model}</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>We consider the problem of change point detection in multiple time series of length $n$ and dimension $p$. Our aim is to partition time into segments, such that in each segment the parameter associated to each time series is constant. For a time series $y$ we write $y = y_{1:n} = (y_1,\dots, y_n) \in(\mathbb R^p)^n$ with $y_i^k$ the $k$-th component of the $p$-dimensional point $y_i\in\mathbb R^p$ in position $i$ in vector $y_{1:n}$. We also use the notation $y _{i:j} = (y_i,\dots, y_j)$ to denote points from index $i$ to $j$. If we assume that there are $M$ change points in a time series, this corresponds to time series splits into $M+1$ distinct segments. Each segment $m \in \{1,\dots, M+1\}$ is generated by independent random variables from a multivariate distribution with  the segment-specific parameter $\theta_m = (\theta_m^1,\dots, \theta_m^p)  \in \mathbb R^p$. A segmentation with $M$ change points is defined by the vector of integers $\tau =(\tau_0 = 0, \tau_1,\dots,\tau_M,\tau_{M+1}=n)$. Segments are given by the sets of indices $\{\tau_i+1,\dots, \tau_{i+1}<span class="sc">\}</span>$ with $i$ in $<span class="sc">\{</span>0,1,\ldots,M<span class="sc">\}</span>$. </span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>We define the set $S_t$ of all possible change point locations related to the segmentation of data points between positions $1$ to $t$ as</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>S_t = <span class="sc">\{</span>\tau = (\tau_0,\tau_1,\dots,\tau_M, \tau_{M+1}) \in \mathbb N^{M+2} | 0=\tau_{0} &lt;\tau_1 &lt; \dots &lt; \tau_M &lt; \tau_{M+1}=t<span class="sc">\}</span>\,.</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>Usually the number of changes $M$ is unknown, and has to be estimated. Many approaches to detecting change points define a cost function for segmentation using the opposite log-likelihood (times two). Here the opposite log-likelihood (times two) linked to data point $y_j$ is given by function $\theta \mapsto \Omega(\theta,y_j)$, where $\theta = (\theta^1,\dots, \theta^p) \in \mathbb R^p$. Over a segment from $i$ to $t$, the parameter remains the same and the segment cost $\mathcal C$ is given by</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>\mathcal C(y_{i:t}) = \min_{\theta \in \mathbb{R}^p} \sum_{j=i}^{t}\Omega(\theta, y_j) = \min_{\theta \in \mathbb{R}^p} \sum_{j=i}^{t} \left(\sum_{k=1}^{p} \omega(\theta^k, y_j^k)\right)\,,</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Cy_it}  </span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>with $\omega$ the atomic likelihood function associated with $\Omega$ for each univariate time series. This decomposition is made possible by the independence hypothesis between dimensions. Notice that function $\omega$ could have been dimension-dependent with a mixture of different distributions (Gauss, Poisson, negative binomial, etc.). In our study, we use the same data model for all dimensions.</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>We consider a penalized version of the cost by a penalty $\beta &gt; 0$, as the zero penalty case would lead to segmentation with $n$ segments. Summing over all segments we end up with a penalty that is linear in the number of segments. Such choice is common in the literature <span class="co">[</span><span class="ot">@yao1988estimating; @Killick</span><span class="co">]</span> although some other penalties have been proposed <span class="co">[</span><span class="ot">@Zhang2007; @Lebarbier2005; @Verzelen2020</span><span class="co">]</span>. The optimal penalized cost associated with our segmentation problem is then defined by</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>    Q_n = \min_{\tau \in S_n} \sum_{i=0}^{M} <span class="sc">\{</span>\mathcal C(y_{(\tau_{i}+1):\tau_{i+1}})+\beta<span class="sc">\}</span>\,.</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Q_n}</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>The optimal segmentation $\tau$ is obtained by the argminimum in @eq-Q_n.</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a><span class="fu">## Functional Pruning Dynamic Programming Algorithm {#sec-UpdateRule}</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>The idea of the Optimal Partitioning (OP) method <span class="co">[</span><span class="ot">@jackson2005algorithm</span><span class="co">]</span> is to search for the last change point defining the last segment in data $y_{1:t}$ at each iteration (with $Q_0 = 0$), which leads to the recursion: </span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>Q_{t} = \min_{i\in<span class="sc">\{</span>0,\dots,t-1<span class="sc">\}</span>}\Big(Q_i + \mathcal C(y_{({i+1}:t}) + \beta \Big)\,.</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>*Functional description.* In the FPOP method we introduce a last segment parameter $\theta = (\theta^1,\dots, \theta^p)$ in $\mathbb R^p$ and define a functional cost $\theta \mapsto Q_t(\theta)$ depending on $\theta$, that takes the following form:</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>Q_t(\theta) = \min_{\tau \in S_t} \Big( \sum_{i=0}^{M-1} <span class="sc">\{</span>\mathcal C(y_{(\tau_{i}+1):\tau_{i+1}})+\beta<span class="sc">\}</span> + \sum_{j=\tau_{M}+1}^{t}\Omega(\theta, y_j) + \beta \Big)\,.</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>As explained in @Maidstone, we can compute the function $Q_{t+1}(\cdot)$ based only on the knowledge of $Q_{t}(\cdot)$ as for each integer $t$ from $0$ to $n-1$. We have:</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>Q_{t+1}(\theta) = \min <span class="sc">\{</span>Q_t(\theta),m_t +\beta <span class="sc">\}</span> + \Omega(\theta, y_{t+1})\,, </span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Q_tpl1}</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>for all $\theta \in \mathbb R^p$, with $m_t = \min_\theta Q_t(\theta)$ and the initialization $Q_0(\theta) = 0$, so that $Q_1(\theta) = \Omega(\theta,y_1)$. By looking closely at this relation, we see that each function $Q_t$ is a piece-wise continuous function consisting of at most $t$ different functions on $\mathbb R^p$, denoted  $q^i_t$:</span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>Q_t(\theta) = \min_{i \in <span class="sc">\{</span>1,\dots,t <span class="sc">\}</span>} \left<span class="sc">\{</span>q_t^i(\theta)\right<span class="sc">\}</span>\,,   </span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>where the $q_t^i$ functions are given by explicit formulas: </span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>q_t^i(\theta) = m_{i-1} + \beta + \sum_{j = i}^{t} \Omega(\theta,y_j)\,,\quad\theta \in \mathbb R^p\,,\quad i = 1,\dots,t.  </span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>and</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>    m_{i-1} =  \min_{\theta \in \mathbb R^p}Q_{i-1}(\theta) = \min_{j \in <span class="sc">\{</span> 1,\dots,i-1<span class="sc">\}</span>}\left<span class="sc">\{</span> \min_{\theta \in \mathbb R^p}q_{i-1}^j(\theta) \right<span class="sc">\}</span>.</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>$${#eq-m_im1}</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>It is important to notice that each $q_t^i$ function is associated with the last change point $i-1$ and the last segment is given by indices from $i$ to $t$. Consequently, the last change point at step $t$ in $y_{1:t}$ is denoted as $\hat\tau_t$ $( \hat \tau_t \le t-1)$ and is given by</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>\hat\tau_t = \underset{i \in <span class="sc">\{</span>1,\dots,t<span class="sc">\}</span>}{Arg\min} \left<span class="sc">\{</span> \min_{\theta \in \mathbb{R}^p} q_t^i(\theta)\right<span class="sc">\}</span>-1.</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>*Backtracking.* Knowing the values of $\hat{\tau}_t$ for all $t=1, \dots, n$, we can always restore the optimal segmentation at time $n$ for $y_{1:n}$. This procedure is called backtracking. The vector $cp(n)$ of ordered change points in the optimal segmentation of $y_{1:n}$ is determined recursively by the relation $cp(n) = (cp(\hat \tau_n), \hat \tau_n)$ with stopping rule $cp(0)=\emptyset$.    </span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>*Parameter space description.* Applying functional pruning requires a precise analysis of the recursion (<span class="co">[</span><span class="ot">-@eq-Q_tpl1</span><span class="co">]</span>) that depends on the property of the cost function $\Omega$. In what follows we consider three choices based on a Gaussian, Poisson, and negative binomial distribution for data generation. The exact formulas of these cost functions are given in @sec-AppendixA.</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>We denote the set of parameter values for which the function $q^i_t(\cdot)$ is optimal as:</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>Z_t^i = \left<span class="sc">\{</span> \theta \in \mathbb R^p|Q_t(\theta) = q_{t}^i(\theta) \right<span class="sc">\}</span>, \quad i = 1,\dots,t.</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>The key idea behind functional pruning is that the $Z_t^i$ are nested ($Z_{t+1}^i \subset Z_t^i$) thus as soon as we can prove the emptiness of one set $Z_t^i$, we delete its associated $q_t^i$ function and do not have to consider its minimum anymore at any further iteration (proof in @sec-geometry). In dimension $p = 1$ this is reasonably easy. In this case, the sets $Z^i_t$ ($i=1,\dots, t$) are unions of intervals and an efficient functional pruning rule is possible by updating a list of these intervals for $Q_t$. This approach is implemented in FPOP <span class="co">[</span><span class="ot">@Maidstone</span><span class="co">]</span>.</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>In dimension $p \ge 2$ it is not so easy anymore to keep track of the emptiness of the sets $Z^i_t$. We illustrate the dynamics of the $Z^i_t$ sets in @fig-Figure1 in the bi-variate Gaussian case. Each color is associated with a set $Z_t^i$ (corresponding to a possible change at $i-1$) for $t$ equal $1$ to $5$. This plot shows in particular that sets $Z_t^i$ can be non-convex.</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>::: {#fig-Figure1} </span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 1 Z sets over time set_seed_617.png)</span> </span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a>The sets $Z^i_t$ over time for the bi-variate independent Gaussian model on time series without change $y = \left( (0.29, 1.93), (1.86, -0.02), (0.9, 2.51), (-1.26, 0.91), (1.22, 1.11)   \right)$. From left to right we represent at time $t=1, 2, 3, 4,$ and $5$ the parameter space $(\theta^1, \theta^2).$ Each $Z^i_t$ is represented by a color. The change $1$ associated with  quadratics $2$ is pruned at $t = 3$. Notice that each time sequence of $Z^i_t$ with $i$ fixed is a nested sequence of sets.</span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a><span class="fu">## Geometric Formulation of Functional Pruning {#sec-geometry}</span></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>To build an efficient pruning strategy for dimension $p \ge 2$ we need to test the emptiness of the sets $Z^i_t$ at each iteration. Note that to get $Z_t^i$ we need to compare the functional cost $q^i_t$ with any other functional cost $q^j_{t}$, $j=1,\dots, t,\, j\neq i$. This leads to the definition of the following sets.</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>::: {#def-defS}</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>We define *$S$-type set* $S^i_j$ using the function $\Omega$ as </span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>S_j^i = \left<span class="sc">\{</span> \theta \in \mathbb R^p \,|\, \sum_{u=i+1}^j \Omega(\theta, y_u) \le m_{j}-m_{i}\right<span class="sc">\}</span>\,,\hbox{ when } i &lt; j</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>and $S_i^i = \mathbb R^p$. We denote the set of all possible S-type sets as $\mathbf S$.</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>To ease some of our calculations, we now introduce some additional notations. For $\theta = (\theta^1,\dots,\theta^p)$ in $\mathbb R^p$, $1 \le i &lt; j \le n$ we define $p$ univariate functions $\theta^k \mapsto s^k_{ij}(\theta^k)$ associated to the $k$-th time series as</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>s^k_{ij}(\theta^k)  = \sum_{u = i+1}^{j} \omega(\theta^k,y_u^k), \quad  k = 1,\dots,p\,.</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>$${#eq-setS}</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>We introduce a constant $\Delta_{ij}$ and a function $\theta \mapsto s_{ij}(\theta)$:</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>    \begin{aligned}</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>       \Delta_{ij} &amp; =  \,m_j - m_{i}\,,<span class="sc">\\</span></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>       s_{ij}(\theta) &amp; =  \sum_{k=1}^p s^k_{ij}(\theta^k)- \Delta_{ij}\,,</span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>    \end{aligned}</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>    \right.</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>$$ {#eq-setSfunc}</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>where $m_{i}$ and $m_j$ are defined as in @eq-m_im1. The sets $S_j^i$ for $i &lt; j$ are also described by relation </span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>S_j^i = s_{ij}^{-1} (-\infty,0]\,.</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>$$ {#eq-setS}</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>In @fig-Figure2 we present the level curves for three different parametric models given by $s_{ij}^{-1} (<span class="sc">\{</span>w<span class="sc">\}</span>)$ with $w$ a real number. Each of these curves encloses an S-type set.</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>::: {#fig-Figure2}</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 2 Contoure of S-type sets and cost .png)</span>{width=90%}</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>Three examples of the level curves of a function $s_{ij}$ for bi-variate time series $<span class="sc">\{</span>x,y<span class="sc">\}</span>$. We use the following simulations for univariate time series : (a) $x\sim \mathcal{N}(0,1)$, $y\sim \mathcal{N}(0,1)$, (b) $x \sim \mathcal{P}(1)$, $y \sim \mathcal{P}(3)$, (c) $x\sim \mathcal{NB}(0.5,1)$, $y\sim \mathcal{NB}(0.8, 1)$.</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>At time $t = 1,\dots, n$ we define the following sets associated to the last change point index $i-1$:</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>-$\mathtt{past}\,\mathtt{set} \,\mathcal P^i$</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>\mathcal P^i =<span class="sc">\{</span>S_{i}^u,\, u = 1,\dots,i-1<span class="sc">\}</span>\,.</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>-$\mathtt{future}\, \mathtt{set} \,\mathcal F^i(t)$ </span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>\mathcal F^i(t) =<span class="sc">\{</span>S_{v}^i, \, v = i,\dots,t<span class="sc">\}</span>\,.</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>We denote the cardinal of a set $\mathcal{A}$ as $|\mathcal{A}|$. Using these two sets of sets, the $Z^i_t$ have the following description.</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>:::{#prp-proposition_sets}</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>At iteration $t$, the functional cost  $Q_t(\cdot)$ defines the subsets  $Z_t^i$ ($i=1,\dots, t$), each of them being the intersection of  the sets in $\mathcal{F}^i(t)$ minus the union of the sets in $\mathcal P^i$. </span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>Z_t^i = (\cap_{S\in \mathcal{F}^i(t)}S) \setminus (\cup_{S\in \mathcal{P}^i}S)\,,\quad i = 1,\dots,t.</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>$${#eq-setsZ}</span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>::: {.proof}</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>Based on the definition of the set $Z_t^i$, the proof is straightforward. Parameter value $\theta$ is in  $Z_t^i$ if and only if $q_t^i(\theta) \le q_t^u(\theta)$ for all $u \ne i$; these inequalities define the past set (when $u &lt; i$) and the future set (when $u&gt;i$). By convention  we assume that, in case $i = t$, $\cap_{S\in \mathcal F^i(t)}S = \mathbb R^p$. </span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a>:::{#cor-col1}</span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a>The sequence $\zeta^i = (Z_t^i)_{t\ge i}$ is a nested sequence of sets.</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>Indeed, $Z_{t+1}^i$ is equal to $Z_t^i$ with an additional intersection in the future set. Based on @cor-col1, as soon as we prove that the set $Z_t^i$, is empty, we delete its associated $q_t^i$ function and, consequently, we can prune the change point $i-1$. In this context, functional and inequality-based pruning have a simple geometric interpretation.</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>*Functional pruning geometry.* The position $i-1$ is pruned at step $t+1$, in $Q_{t+1}(\cdot),$ if the intersection set of $\cap_{S\in \mathcal F^i(t)}S$ is covered by the union set $\cup_{S\in \mathcal P^i}S$.</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>*Inequality-based pruning geometry.* The inequality-based pruning of PELT is equivalent to the geometric rule: position $i-1$ is pruned at step $t+1$ if the set $S_t^i$ is empty. In that case, the intersection  set $\cap_{S\in \mathcal F^i(t)}S$ is empty, and therefore $Z_t^i$ is also empty using @eq-setsZ. This shows that if a change is pruned using inequality-based pruning it is also pruned using functional pruning. For the dimension $p =1$ this claim was theoretically proved in @Maidstone.</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>The construction of set $Z^i_t$ using @prp-proposition_sets is illustrated in @fig-Figure3 for a bi-variate independent Gaussian case: we have the intersection of three S-type sets and the subtraction of three S-type sets.</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure3} </span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 3 Bilding Z with 3 past and 3 future disks set_seed_21.png)</span>{width=50%}</span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>Examples of building a set $Z^i_t$ with $\lvert\mathcal P^i\rvert = \lvert\mathcal F^i(t)\rvert = 3$ for the Gaussian case in 2-D ($\mu = 0,\sigma=1$). The green disks are S-type sets of the past set $\mathcal P^i$. The blue disks are  S-type sets of the future set $\mathcal{F}^i(t)$.</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a><span class="fu"># Geometric Functional Pruning Optimal Partitioning {#sec-GeomFPOP}</span></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a><span class="fu">## General Principle of GeomFPOP{#sec-principle}</span></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a>Rather than considering an exact representation of the $Z^i_t$, our idea is to consider a hopefully slightly larger set that is easier to update. To be specific, for each $Z^i_t$ we introduce $\tilde{Z}^i_t$, called *testing set*, such that $Z^i_t\subset \tilde{Z}^i_t$. If at time $t$ $\tilde{Z}^i_t$ is empty thus is $Z^i_t$ and thus change $i-1$ can be pruned. From @prp-proposition_sets we have that starting from $Z = \mathbb{R}^p$ the set $Z^i_t$ is obtained by successively applying two types of operations: intersection with an S-type set  $S$ $(Z\cap S)$ or subtraction of an S-type set $S$ $(Z\setminus S)$. Similarly, starting from $\tilde{Z} = \mathbb{R}^p$ we obtain $\tilde{Z}^i_t$ by successively applying approximation of these intersection and subtraction operations. Intuitively, the complexity of the resulting algorithm is a combination of the efficiency of the pruning and the easiness of updating the testing set. </span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>*A Generic Formulation of GeomFPOP.* In what follows we will generically describe GeomFPOP, that is, without specifying the precise structure of the testing set $\tilde{Z}^i_t$. We call $\widetilde{\mathbf{Z}}$ the set of all possible $\tilde{Z}^i_t$ and assume the existence of two operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$. We have the following assumptions for these operators.</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a>:::{#def-assumption1}</span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>The two operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$ are such that:</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>the left input is a $\tilde{Z}$-type set (that is an element of $\widetilde{\mathbf{Z}}$);</span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>the right input is a $S$-type set;</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>the output is a $\tilde{Z}$-type set;</span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a><span class="ss">  4. </span>$\tilde{Z} \cap S \subset \tilde{Z} \cap_{\tilde{Z}} S$ and $\tilde{Z} \setminus S \subset \tilde{Z} \setminus_{\tilde{Z}} S$.</span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>We give a proper description of two types of testing sets and their approximation operators in @sec-approximation.</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>At each iteration $t$ GeomFPOP will construct $\tilde{Z}^i_{t+1}$ from $\tilde{Z}^i_{t}$, $\mathcal P^i$ and, $\mathcal F^i(t)$ iteratively using the two operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$. To be specific, we define $S_j^F$ the j-th element of $\mathcal F^i(t)$ and $S_P^j$ the j-th element of $\mathcal P^i$, we use the following iteration:</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>      \begin{aligned}</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a>       A_{0} =\tilde{Z}^i_{t} \,, &amp; \quad A_j = A_{j-1}\,\cap_{\tilde{Z}}\, S_j^F\,, &amp; j = 1,\dots , |\mathcal{F}^i(t)|\,,<span class="sc">\\</span></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a>        B_{0} =A_{|\mathcal{F}^i(t)|}\,, &amp; \quad B_j = B_{j-1}\,\setminus_{\tilde{Z}} \, S_P^j\,, &amp; j = 1,\dots , |\mathcal{P}^i| \,,<span class="sc">\\</span></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>    \end{aligned}  </span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a>and define $\tilde{Z}^i_{t+1} = B_{|\mathcal P^i|}.$ Using the fourth property of @def-assumption1 and @prp-proposition_sets, we get that at any time of the algorithm $\tilde{Z}^i_t$ contains ${Z}^i_t.$</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a>The pseudo-code of this procedure is described in Algorithm 1. The $\mathtt{select}(\mathcal{A})$ step in Algorithm 1, where $\mathcal{A} \subset  \mathbf S$, returns a subset of $\mathcal{A}$ in  $\mathbf S$. By default, $\mathtt{select}(\mathcal{A}) := \mathcal{A}$.</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a><span class="in">```pseudocode</span></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Geometric update rule of $\tilde{Z}^i_t$}</span></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a><span class="in">\Procedure{updateZone}{$\tilde{Z}_{t-1}^i, \mathcal{P}^i, \mathcal{F}^i(t), i, t$}</span></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a><span class="in">  \State $\tilde{Z}_t^i \gets \tilde{Z}_{t-1}^i$</span></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a><span class="in">  \For{$S \in \mathtt{select} (\mathcal{F}^i(t))$}</span></span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\tilde{Z}_t^i \gets \tilde{Z}_t^i\cap_{\tilde{Z}} S$</span></span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a><span class="in">  \EndFor</span></span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a><span class="in">  \For{$S \in \mathtt{select} (\mathcal{P}^i)$}</span></span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\tilde{Z}_t^i \gets \tilde{Z}_t^i \setminus_{\tilde{Z}} S$</span></span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a><span class="in">  \EndFor</span></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a><span class="in">  \Return $\tilde{Z}_t^i$</span></span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a><span class="in">\EndProcedure</span></span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>We denote the set of candidate change points at time $t$ as $\tau_t$. Note that for any $(i-1)\in \tau_t$  the sum of $|\mathcal P^i|$ and $|\mathcal F^i(t)|$ is $|\tau_t|$. With the default $\mathtt{select}()$ procedure we do $\mathcal{O}(p|\tau_t|)$ operations in Algorithm 1. By limiting the number of elements returned by $\mathtt{select}()$ we can reduce the complexity.</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a>:::{.remark}</span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a>For example, if the operator $\mathcal{A} \mapsto \mathtt{select}(\mathcal{A})$, regardless of $|\mathcal A|$, always returns a subset of constant size, then the overall complexity of GeomFPOP is at worst equal to that of PELT with $\sum_{t=1}^{n}\mathcal{O}(p|\tau_t|)$ time complexity.</span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a>Using this $\mathtt{updateZone}()$ procedure we can now informally describe the GeomFPOP algorithm. At each iteration the algorithm will:</span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>find the minimum value for $Q_t$, $m_t$ and the best position for last change point $\hat \tau_t$ (note that this step is standard: as in the PELT algorithm we need to minimize the cost of the last segment defined in @eq-Cy_it);</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>compute all sets $\tilde{Z}_{t}^{i}$ using $\tilde{Z}_{t-1}^{i}$, $\mathcal{P}^i$, and $\mathcal{F}^i(t)$ with the $\mathtt{updateZone}()$ procedure;</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>remove changes such that $\tilde{Z}_{t}^{i}$ is empty.</span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>To simplify the pseudo-code of GeomFPOP, we also define the following operators:</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathtt{bestCost<span class="sc">\&amp;</span>Tau}(t)$ operator returns two values: the minimum value of $Q_t$, $m_t$, and the best position for last change point $\hat \tau_t$ at time $t$ (see @sec-UpdateRule); </span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\mathtt{getPastFutureSets}(i,t)$ operator returns a pair of sets ($\mathcal F^i(t)$, $\mathcal P^i$) for change point candidate $i-1$ at time $t\,$;</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>$\mathtt{backtracking}(\hat\tau, n)$ operator returns the optimal segmentation for $y_{1:n}$. </span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>The pseudo-code of GeomFPOP is presented in Algorithm 2.</span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>:::{#alg2}</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a><span class="in">```pseudocode</span></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{GeomFPOP algorithm}</span></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a><span class="in">\Procedure{GeomFPOP}{$y, \Omega(\cdot, \cdot),\beta$}</span></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a><span class="in">  \State $m_0 \gets 0,\quad Q_0(\theta) \gets 0\,,\quad \tau_0 \gets \emptyset, \quad \{\tilde{Z}^{i}_{i-1}\}_{i\in \{1,\dots,n\}}\gets  \mathbb{R}^p$</span></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a><span class="in">  \For{$t = 1, \dots, n$}</span></span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $Q_t(\theta) \gets \min \{ Q_{t-1}(\theta), m_{t-1} + \beta\} + \Omega(\theta, y_t)$</span></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $(m_t, \hat\tau_t) \gets \mathtt{bestCost\&amp;Tau}(t)$</span></span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a><span class="in">    \For{$i-1 \in \tau_t$}</span></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a><span class="in">      \State $(\mathcal{P}^i, \mathcal{F}^i(t)) \gets \mathtt{getPastFutureSets}(i,t)$</span></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a><span class="in">      \State $\tilde{Z}_t^i \gets \mathtt{updateZone}(\tilde{Z}_{t-1}^i, \mathcal{P}^i, \mathcal{F}^i(t), i, t)$</span></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a><span class="in">      \If{$\tilde{Z}_t^i = \emptyset$}</span></span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a><span class="in">        \State $\tau_t \gets \tau_t \backslash\{i-1\}$</span></span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a><span class="in">      \EndIf</span></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a><span class="in">    \EndFor</span></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a><span class="in">    \State $\tau_t \gets (\tau_{t-1}, t-1)$</span></span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a><span class="in">  \EndFor</span></span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a><span class="in">  \Return $cp(n) \gets \mathtt{backtracking}(\hat\tau, n)$</span></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a><span class="in">\EndProcedure</span></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a><span class="fu"># Approximation Operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$ {#sec-approximation}</span></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>The choice of the geometric structure and the way it is constructed directly affects the computational cost of the algorithm. We consider two types of testing set $\tilde{Z} \in \widetilde{\mathbf{Z}}$, a S-type set $\tilde{S}\in \mathbf{S}$ (see @def-defS) and a hyperrectangle $\tilde{R}\in  \mathbf{R}$ defined below.</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a>:::{#def-Hyperrectangle}</span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a>Given two vectors in $\mathbb{R}^p$, $\tilde{l}$ and $\tilde{r}$ we define the set $\tilde{R}$, called *hyperrectangle*, as:</span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a>\tilde{R} = <span class="co">[</span><span class="ot">\tilde{l}_1,\tilde{r}_1</span><span class="co">]</span>\times \dots \times<span class="co">[</span><span class="ot">\tilde{l}_p,\tilde{r}_p</span><span class="co">]</span>\,. <span class="sc">\\</span></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>We denote the set of all possible sets $\tilde{R}$ as $\mathbf{R}$.</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a>To update the testing sets we need to give a strict definition of the operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$ for each type of testing set. To facilitate the following discussion, we rename them. For the first type of geometric structure, we rename the testing set $\tilde{Z}$ as $\tilde{S}$, the operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$ as $\cap_{S}$ and $\setminus_{S}$ and $\tilde{Z}$-type approximation as S-type approximation. And, likewise, we rename the testing set $\tilde{Z}$ as $\tilde{R}$, the operators $\cap_{\tilde{Z}}$ and $\setminus_{\tilde{Z}}$ as $\cap_{R}$ and $\setminus_{R}$ and $\tilde{Z}$-type approximation as R-type approximation for the second type of geometric structure.</span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a><span class="fu">## S-type Approximation</span></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a>With this approach, our goal is to keep track of the fact that at time $t = 1,\dots, n$ there is a pair of changes $(u_1,u_2)$, with $u_1 &lt; i &lt; u_2\le t$ such that $S^i_{u_2}\subset S^{u_1}_{i}$ or there is a pair of changes $(v_1,v_2)$, with $i  &lt; v_1 &lt; v_2\le t$  such that $S^i_{v_1}\cap S^i_{v_2}$ is empty. If at time $t$ at least one of these conditions is met, we can guarantee that the set $\tilde{S}$ is empty, otherwise, we propose to keep as the result of approximation the last future S-type set $S^i_t$, because it always includes the set $Z^i_t$. This allows us to quickly check and prove (if $\tilde{S} =\emptyset$) the emptiness of set $Z^i_t$.</span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>We consider two generic S-type sets, $S$ and $\tilde{S}$  from $\mathbf{S}$, described as in @def-defS by the functions $s$ and  $\tilde{s}$:</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>s(\theta) = \sum_{k=1}^p s^k(\theta^k)- \Delta\,,\quad\quad \tilde{s}(\theta) = \sum_{k=1}^p {\tilde{s}}^{k}(\theta^k)- \tilde{\Delta}\,. </span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a>:::{#def-def_oper_S}</span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a>For all  $S$ and $\tilde{S}$ in $\mathbf{S}$ we define the operators $\cap_{S}$ and $\setminus_{S}$ as:</span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a>        &amp;\tilde{S}\, \cap_{S}\, S&amp; = \left<span class="sc">\{</span></span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>        \begin{aligned}</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>            &amp; \emptyset \,,  &amp; \hbox{ if }  \tilde{S}\cap S = \emptyset \,,<span class="sc">\\</span></span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a>            &amp; \tilde{S}\,, &amp; \hbox{otherwise}\,.<span class="sc">\\</span></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a>        \end{aligned} </span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a>        \right.<span class="sc">\\</span></span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a>         &amp;\tilde{S} \,\setminus_{S}\, S   &amp; = \left<span class="sc">\{</span></span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a>        \begin{aligned}</span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a>        &amp; \emptyset \,,  &amp; \hbox{ if }  \tilde{S} \subset S\,,<span class="sc">\\</span></span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a>        &amp; \tilde{S}\,, &amp; \hbox{otherwise}\,.<span class="sc">\\</span></span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a>        \end{aligned} </span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a>        \right.</span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a>    \end{aligned}    </span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a>As a consequence, we only need an easy way to detect any of these two geometric configurations: $\tilde{S}\cap S$ and $\tilde{S} \subset S$.</span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a>In the Gaussian case, the S-type sets are $p$-balls and an easy solution exists based on comparing radii (see @sec-AppendixB for details). In the case of other models (as Poisson or negative binomial), intersection and inclusion tests can be performed based on a solution using separative hyperplanes and iterative algorithms for convex problems (see @sec-AppendixC). We propose another type of testing set solving all types of models with the same method.</span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a><span class="fu">## R-type Approximation</span></span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a>Here, we approximate the sets $Z^i_t$ by hyperrectangles $\tilde{R}^i_t \in \mathbf{R}$. A key insight of this approximation is that given a hyperrectangle $R$ and an S-type set $S$ we can efficiently (in $\mathcal{O}(p)$ using @prp-prop_solution_rect) recover the best hyperrectangle approximation of $R \cup S$ and $R \setminus S.$ Formally we define these operators as follows.</span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>:::{#def-operR}</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a>For all  $R, \tilde{R} \in \mathbf{R}$ and $S\in \mathbf{S}$  we define the operators $\cap_{R}$ and $\setminus_{R}$ as:</span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a>    \begin{aligned}</span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a>     R \cap_R S = \cap_{<span class="sc">\{</span>\tilde{R} | R \cap S \subset \mathbf{R}<span class="sc">\}</span>} \tilde{R}\,,<span class="sc">\\</span></span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a>      R \setminus_R S = \cap_{<span class="sc">\{</span>\tilde{R} | R \setminus S \subset \mathbf{R}<span class="sc">\}</span>} \tilde{R}\,.</span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>We now explain how we compute these two operators. First, we note that they can be recovered by solving a $2p$ one-dimensional optimization problems.</span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>:::{#prp-proposition}</span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a>The $k$-th minimum coordinates $\tilde{l}_k$ and maximum coordinates $\tilde{r}_k$ of   $\tilde{R} = R \cap_R S$ (resp. $\tilde{R} = R \setminus_R S$) is obtained as</span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a>\tilde{l}_k \hbox{ or } \tilde{r}_k = </span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a>    \begin{aligned}</span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>        &amp;\min_{\theta_k \in \mathbb{R}} \hbox{ or } \max_{\theta_k \in \mathbb R}  \theta_k\,,<span class="sc">\\</span></span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a>        &amp; \hbox{subject to } \varepsilon s(\theta) \le 0 \,,<span class="sc">\\</span></span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a>        &amp; \quad \quad \quad \quad \quad l_j \le \theta_j \le r_j\,,\quad j = 1,\dots,p \,,<span class="sc">\\</span></span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a>    \end{aligned}</span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a>\right.      </span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>$${#eq-inclusionOptim}</span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>with $\varepsilon = 1$ (resp. $\varepsilon = -1$).</span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>To solve the previous problems ($\varepsilon = 1$ or $-1$), we define the following characteristic points.</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a>:::{#def-points}</span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>Let $S \in \mathbf S$, described by function $s(\theta) = \sum_{k=1}^{p} s^k(\theta^k) - \Delta$ from the family of functions (<span class="co">[</span><span class="ot">-@eq-setSfunc</span><span class="co">]</span>), with $\theta\in \mathbb R^p$. We define the *minimal point* $\mathbf{c}\in \mathbb R^p$ of $S$ as:</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a>\mathbf{c} = \left<span class="sc">\{</span>\mathbf{c}^k\right<span class="sc">\}</span>_{k=1,\dots,p}, \quad \text { with }\quad \mathbf{c}^k =\underset{\theta^k \in \mathbb R} {Arg\min} <span class="sc">\{</span> s^k(\theta^k) <span class="sc">\}</span>\,.</span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a>$$ {#eq-c}</span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>Moreover, with $R \in \mathbf R$ defined through vectors $l,r \in \mathbb R^p$, we define two points of $R$, the *closest point* $\mathbf{m} \in \mathbb R^p$ and the *farthest point* $\mathbf M \in \mathbb R^p$ relative to $S$ as </span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a>    \mathbf{m} =\left<span class="sc">\{</span>\mathbf{m}^k\right<span class="sc">\}</span>_{k=1,\dots,p},\quad \text { with }\quad </span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>    \mathbf{m}^k = \underset{l^k \le \theta^k \le r^k}{Arg\min}  \left<span class="sc">\{</span> s^k(\theta^k)\right<span class="sc">\}</span>,<span class="sc">\\</span></span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a>    \mathbf{M} =\left<span class="sc">\{</span>\mathbf{M}^k\right<span class="sc">\}</span>_{k=1,\dots,p},\quad \text { with }\quad </span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a>    \mathbf{M}^k = \underset{l^k \le \theta^k \le r^k}{Arg\max}  \left<span class="sc">\{</span>s^k(\theta^k)\right<span class="sc">\}</span>\,.</span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a>    \end{aligned}</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a>:::{.remark}</span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a>In the Gaussian case, $S$ is a ball in $\mathbb R^p$ and </span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{c}$ is the center of the ball;</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{m}$ is the closest point to $\mathbf{c}$ inside $R$;</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{M}$ is the farthest point to $\mathbf{c}$ in $R$.</span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure4}  </span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 4 Minimal closest and farthest points.png)</span>{width=90%}</span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a>Three examples  of minimal point $\mathbf{c}$, closest point $\mathbf{m}$ and farthest point $\mathbf{M}$ for bi-variate Gaussian case: (a) $R \subset S$; (b) $R \cap S \neq \emptyset$; (c) $R \cap S = \emptyset$.</span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a>:::{#prp-prop_solution_rect}</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a>Let  $\tilde{R} = R \cap_{R} S$ (resp. $R\setminus_{R} S$), with $R \in \mathbf{R}$ and $S \in \mathbf{S}$. We compute the boundaries $(\tilde{l}, \tilde{r})$ of $\tilde{R}$ using the following rule:</span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We define the point $\tilde{\theta}\in \mathbb{R}^p$ as the closest point $\mathbf{m}$ (resp. farthest $\mathbf{M}$). For all $k = 1,\dots p$ we find the roots $\theta^{k_1}$ and $\theta^{k_2}$ of the one-variable $(\theta^k)$ equation </span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a>s^k(\theta^k)+\sum_{j\neq k} s^j(\tilde{\theta}^j) -\Delta= 0 \,.</span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a>If the roots are real-valued we consider that $\theta^{k_1} \le \theta^{k_2}$, otherwise we write $\Big<span class="co">[</span><span class="ot">\theta^{k_1},\theta^{k_2}\Big</span><span class="co">]</span> = \emptyset$.</span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We compute the boundary values $\tilde{l}^k$ and $\tilde{r}^k$ of </span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a>$\tilde{R}$ as:</span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For $R\cap_{R} S$ $(k = 1,\dots,p)$:</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a>\Big<span class="co">[</span><span class="ot">\tilde{l}^k,\tilde{r}^k\Big</span><span class="co">]</span> = \Big<span class="co">[</span><span class="ot">\theta^{k_1},\theta^{k_2}\Big</span><span class="co">]</span> \cap \Big<span class="co">[</span><span class="ot">l^k, r^k\Big</span><span class="co">]</span>\,.</span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a>$$  {#eq-updateIntersection}</span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For $R\setminus_{R} S$ $(k = 1,\dots,p)$:</span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a>\Big<span class="co">[</span><span class="ot">\tilde{l}^k,\tilde{r}^k\Big</span><span class="co">]</span> =</span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a>&amp; \Big<span class="co">[</span><span class="ot">l^k, r^k\Big</span><span class="co">]</span>  \setminus \Big<span class="co">[</span><span class="ot">\theta^{k_1},\theta^{k_2}\Big</span><span class="co">]</span> \,,  &amp; \hbox{if} \quad \Big<span class="co">[</span><span class="ot">\theta^{k_1},\theta^{k_2}\Big</span><span class="co">]</span> \not\subset \Big<span class="co">[</span><span class="ot">l^k, r^k\Big</span><span class="co">]</span>\,,<span class="sc">\\</span></span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a>&amp; \Big<span class="co">[</span><span class="ot">l^k, r^k\Big</span><span class="co">]</span>\,, &amp; \hbox{otherwise}\,.<span class="sc">\\</span></span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a>If there is a dimension $k$ for which $\Big<span class="co">[</span><span class="ot">\tilde{l}^k, \tilde{r}^k\Big</span><span class="co">]</span>=\emptyset$, then the set $\tilde{R}$ is empty.</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a>The proof of @prp-prop_solution_rect is presented in @sec-AppendixD. </span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a><span class="fu"># Simulation Study of GeomFPOP {#sec-study}</span></span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a>In this section, we study  the efficiency of GeomFPOP using simulations of multivariate independent time series. For this, we implemented GeomFPOP (with S and R types) and PELT for the Multivariate Independent Gaussian Model in the R-package 'GeomFPOP'  <span class="co">[</span><span class="ot">https://github.com/lpishchagina/GeomFPOP</span><span class="co">](https://github.com/lpishchagina/GeomFPOP)</span>   written in R/C++. By default, the value of penalty $\beta$ for each simulation was defined by the Schwarz Information Criterion proposed in @Yao ($\beta = 2p \log{n}$).</span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a>*Overview of our simulations.* First, as a quality control we made sure that the output of PELT and GeomFPOP were identical on a number of simulated profiles. Second, we studied cases where the PELT approach is not efficient, that is when the data has no or few changes relative to $n$. Indeed, it was shown in @Killick and @Maidstone that the run time of PELT is close to $\mathcal{O}(n^2)$ in such cases. So we considered simulations of multivariate time series without change (only one segment). By these simulations we evaluated the pruning efficiency of GeomFPOP (using S and R types) for  dimension $2\le p\le 10$ (see @fig-Figure5 in @sec-NC). For small dimensions ($2 \le p \le 4$)  we also evaluated  the run time of GeomFPOP and  PELT and compare them (see @fig-Figure6 in  @sec-TCsmall).  In addition, we considered  another approximation of the $Z^i_t$ where we applied our $\cap_{R}$ and $\setminus_R$ operators only for a randomly selected subset of the past and future balls. In practice, this strategy turned out to be faster computationally than the full/original GeomFPOP and PELT (see @fig-Figure7 in @sec-GeomFPOP_random). For this strategy we also generated time series of a fixed size ($10^6$ data points) and varying number of segments and evaluated how the run time vary with the number of segments for small dimensions ($2 \le p \le 4$). Our empirical results confirmed that the GeomFPOP (R-type: $\mathtt{random/random}$) approach is computationally comparable to PELT when the number of changes is large (see @fig-Figure9 in @sec-Run_time_segment_nb).</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Number of Change Point Candidates stored over Time {#sec-NC}</span></span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a>We evaluate the functional pruning efficiency of the GeomFPOP method using simulations with $10^4$ data points (without change, i.e. i.i.d $\mathcal{N}_p(0, I_p)$). For such signals, PELT typically does not pruned (e.g. for $t=10^4$, $p=2$ it stores almost always $t$ candidates).</span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a>We report in @fig-Figure5 the percentage of candidates that are kept by GeomFPOP as a function of $n$, $p$ and the type of pruning (R or S). Regardless of the type of approximation and contrary to PELT, we observe that there is some pruning. However when increasing the dimension $p$, the quality of the pruning decreases. </span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a>Comparing  @fig-Figure5 left and the right we see that for dimensions $p=2$ to $p=5$ R-type prunes more than the S-type, while for larger dimensions the S-type  prunes more than the R-type. For example, for $p = 2$ at time $t=10^4$  by GeomFPOP (R-type) the number of candidates stored over $t$  does not exceed $1\%$ versus $3\%$ by GeomFPOP (S-type). This intuitively makes sense. One the one hand the R-type approximation of a sphere gets worst with the dimension. On the other hand with R-type approximation every new approximation is included in the previous one. For small dimensions this memory effect outweight the roughness of the approximation.</span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure5}  </span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 5 Number of candidates.png)</span>{width=80%}</span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a>Percentage of candidate change points stored over time by GeomFPOP with R (left) or S (right) type pruning for dimension $p = 2,\dots, 10$. We simulated 100 i.i.d Gaussian data $\mathcal{N}_p(0, I_p)$ and report the average.</span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a>Based on these results we expect that R-type pruning GeomFPOP will be more efficient than S-type pruning for small dimensions.</span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a><span class="fu">## Empirical Time Complexity of GeomFPOP {#sec-TCsmall}</span></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a>We studied the run time of GeomFPOP (S and R-type) and compared it to PELT for small dimensions ($p=2, 3, 4$).</span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a>Run times were limited to three minutes and were recorded for simulations (without change, i.e i.i.d $\mathcal{N}_p(0, I_p)$). The results are presented in @fig-Figure6. We observe that GeomFPOP is faster than PELT only for $p=2$. For $p=3$ run times are comparable and for $p=4$ GeomFPOP is slower. This lead us to consider a randomized version of GeomFPOP (see next subsection).</span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure6}  </span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 6 Time complexity PELT GeomFPOP small p.png)</span>{width=90%}</span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a>Run time of GeomFROP (S and R types) and PELT using multivariate time series without change points. The maximum run time of the algorithms is 3 minutes. Averaged over $100$ data sets.</span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a><span class="fu">## Empirical Time Complexity of a Randomized GeomFPOP {#sec-GeomFPOP_random}</span></span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a>R-type GeomFPOP is designed in such a way that at each iteration we need to consider all past and future spheres of change $i$. In practice, it is often sufficient to consider just a few of them to get an empty set. Having this in mind, we propose a further approximation of the $Z^i_t$ where we apply our $\cap_{R}$ and $\setminus_R$ operators only for a randomly selected subset of the past and future sets. In detail, we propose to redefine the output of the $\mathtt{select}()$ function in Algorithm 1 for any sets $\mathcal{P}^i$ and $\mathcal{F}^i(t)$ as:</span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathtt{select}(\mathcal{P}^i)$ returns one random set from $\mathcal{P}^i$.</span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathtt{select}(\mathcal{F}^i(t))$  returns the last set $S^i_t$ and one random  set from $\mathcal{F}^i(t)$.</span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a>Thus, we consider the following geometric update rule:</span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$(\mathtt{random / random})$ At time $t$ we update hyperrectangle:</span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>by only two intersection operations: one with the last S-type set $S^i_t$ from $\mathcal{F}^i(t)$, and one with a random  S-type set from $\mathcal{F}^i(t)$;</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>by only one exclusion operation with a random  S-type set from $\mathcal{P}^i$.</span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a>In this approach at time $t$ we do no more than three operations to update the testing set $\tilde{Z}^i_t$ for each $(i-1) \in \tau_t$. Even with large values of  $p$, the overall complexity of GeomFPOP should not be worse than that of PELT. We investigated other randomized strategies but this simple one was sufficient to significantly improve run times. The run time of our optimization approach and PELT in dimension ($p= 2, \dots, 10, 100$) are presented in  @fig-Figure7. </span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>As in @sec-TCsmall, run times were limited to three minutes and were recorded for simulations of length ranging from $2^{10}$ to $2^{23}$ data points (without change, i.e i.i.d $\mathcal{N}_p(0, I_p)$).</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a>Although the $\mathtt{(random/random)}$ approach reduces the quality of pruning (see @sec-AppendixE), it gives a significant gain in run time compared to PELT in small dimensions. To be specific, with a run time of five minutes GeomFPOP, on average,   processes a time series with a length of about $8\times 10^6$, $10^6$  and $2,5\times 10^5$ data points in the dimensions $p=2,3$ and $4$, respectively. At the same time, PELT manages to process time series with a length of at most $6,5\times10^4$ data points in these dimensions. </span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure7} </span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 7 Time complexity PELT GeomFPOP p_2_10_100.png)</span>{width=90%}</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a>Run time of the $\mathtt{(random/random)}$ approach of { GeomFPOP} (R-type) and PELT using p-variate time series without change points ($p=2,\dots, 10,100$). The maximum run time of the algorithms is 3 minutes. Averaged over $100$ data sets.</span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a><span class="fu">## Empirical Complexity of the Algorithm as a Function of $p$ {#sec-Run_time_p}</span></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a>We also evaluate the slope coefficient $\alpha$ of the run time curve of GeomFPOP with random sampling of the past and future candidates for all considered dimensions. In @fig-Figure8 we can see that already for $p\ge 7$ $\alpha$ is close to $2$.</span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure8}  </span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 8 estimation alpha p_2_10_100.png)</span>{width=80%}</span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a>Run time dependence of $\mathtt{(random/random)}$ approach of GeomFPOP (R-type) on dimension $p$.</span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a><span class="fu">## Run Time as a Function of the Number of Segments {#sec-Run_time_segment_nb}</span></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a>For small dimensions ($2\le p \le 4$) we also generated time series with $10^6$ data points with increasing number of segments. We have considered the following number of segments: $(1,2,5) \times 10^i$( for $i=0,\dots,3$) and $10^4$. The mean was equal to $1$ for even segments,  and $0$ for odd segments. In @fig-Figure9 we can see the run time dependence of the $\mathtt{(random/random)}$ approach of GeomFPOP (R-type) and PELT on the number of segments for this type of time series. Interestingly, the run time of GeomFPOP $\mathtt{(random/random)}$ is comparable to PELT even when the number of segment is large. For smaller number of segments (as already observed) GeomFPOP $\mathtt{(random/random)}$ is an order of magnitude faster.</span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a>:::{#fig-Figure9}  </span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 9 Time complexity Change dependence .png)</span>{width=90%}</span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a>Run time dependence of $\mathtt{(random/random)}$ approach of GeomFPOP (R-type) on the number of segments in time series with $10^6$ data points.</span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowledgments {.unnumbered}</span></span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a>We thank Paul Fearnhead for fruitful discussions.</span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a><span class="fu"># Supplements</span></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a><span class="fu">## Examples of Likelihood-Based Cost Functions  {#sec-AppendixA}</span></span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a>We define a cost function for segmentation as in @eq-Cy_it by the function $\Omega(\cdot,\cdot)$ (the opposite log-likelihood (times two)). Below is the expression of this function  linked to data point $y_i = (y_i^1,\dots, y_i^p)\in \mathbb R^p$ for three examples of Parametric Multivariate Models:</span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a>$$      </span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a>\Omega(\theta,y_i)=</span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a>&amp; \sum_{k=1}^p (y_i^k -\theta^k)^2\,, &amp; \text{ if }y_i \sim \mathcal{N}_p(\theta, \sigma^2\mathbb{I}_p)\,,<span class="sc">\\</span></span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a>&amp;2 \sum_{k=1}^p \left<span class="sc">\{</span>\theta^k-\log\left(\frac{(\theta^k)^{y^k_i}}{y^k_i!}\right)\right<span class="sc">\}</span>\,, &amp; \text{ if }y_i \sim \mathcal{P}(\theta)\,,<span class="sc">\\</span> </span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a>&amp;-2 \sum_{k=1}^p\log\left((\theta^k)^{y_i^k}(1-\theta^k)^\phi \begin{pmatrix}</span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a>y_i^k+\phi-1 <span class="sc">\\</span></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a>y_i^k </span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a>\end{pmatrix}\right)\,,&amp; \text{ if }y_i \sim \mathcal{NB}(\theta,\phi)\,.<span class="sc">\\</span> </span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a>$${#eq-MLE}</span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a>We suppose that the over-dispersion parameter $\phi$ of the Multivariate Negative Binomial distribution is known.</span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a><span class="fu">## Arrangement of Two $p$-balls in $\mathbb R^p$ {#sec-AppendixB}</span></span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a>We define two $p$-balls, $S$ and $S'$ in $\mathbb R^p$ using their centers $c$, $c' \in \mathbb R^p$ and radius $R$, $R' \in \mathbb R^{+}$ as</span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a>S = <span class="sc">\{</span> x \in \mathbb R^p,\lvert\lvert x - c\rvert\rvert ^2 \le R^2<span class="sc">\}</span>\text{ and }S' = <span class="sc">\{</span> x \in \mathbb R^p,\lvert\lvert x - c'\rvert\rvert ^2 \le R'^2<span class="sc">\}</span>,</span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a>where $\lvert\lvert x - c\rvert\rvert ^2 = \sum_{k=1}^p (x^k - c^k)^2$, with $x = (x^1,..., x^p) \in \mathbb R^p$, is the Euclidean norm. The distance between centers $c$ and $c'$ is defined as $d(c, c') = \sqrt{\lvert\lvert c - c' \rvert\rvert^2}$. We have the following simple results:</span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a>S \cap S' = \emptyset \iff d(c,c') &gt; R + R'\,,</span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a>S \subset S' \hbox{ or } S' \subset S \iff d(c,c') \le |R-R'|\,.</span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intersection and Inclusion Tests {#sec-AppendixC}</span></span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a>:::{.remark}</span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a>For any $S^i_j \in \mathbf{S}$ its associated function $s$ can be redefine after normalization by constant $j-i+1$ as:</span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a>s(\theta) = a(\theta) +  \langle b,\theta \rangle + c,</span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a>with $a(\cdot)$ is some convex function depending on $\theta$, $b=<span class="sc">\{</span>b^k<span class="sc">\}</span>_{k =1,\dots, p} \in \mathbb{R}^p$ and $c \in \mathbb{R}$.</span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a>For example, in the Gaussian case, the elements have the following form:</span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a>&amp; a: \theta \mapsto \theta^2\,,&amp; &amp;b^k =  2\bar Y_{i:j}^k\,,&amp;&amp;c =\bar Y^2_{i:j} - \Delta_{ij}\,,</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a>where $\bar Y_{i:j}^k = \frac{1}{j-i+1}\sum_{u=i+1}^j y_u^k$ and $\bar Y^2_{i:j} = \frac{1}{j-i+1}\sum_{u=i+1}^j \sum_{k=1}^p (y_u^k)^2$.</span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a>:::{#def-app:func_h}</span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a>For all  $\theta \in \mathbb R^p$ and $S_1, S_2 \in \mathbf{S}$ with their associated functions, $s_1$ and $s_2$, we define a function $h_{12}$ and a hyperplane $H_{12}$ as:</span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a>&amp;h_{12}(\theta):= s_2(\theta) - s_1(\theta)\,,&amp; &amp;H_{12} := \left <span class="sc">\{</span>\theta \in \mathbb{R}^p | h_{12}(\theta) = 0 \right <span class="sc">\}</span>\,.</span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a>We denote by $H_{12}^+ := <span class="sc">\{</span>\theta \in \mathbb{R}^p |h_{12}(\theta)&gt; 0<span class="sc">\}</span>$ and $H_{12}^- := <span class="sc">\{</span>\theta \in \mathbb{R}^p |h_{12}(\theta)&lt; 0<span class="sc">\}</span>$ the positive and negative half-spaces of $H_{12}$, respectively. We call $\mathbf{H}$ the set of hyperplanes.</span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a>For all $S \in \mathbf{S}$ and $H \in \mathbf{H}$ we introduce a $\mathtt{half-space}$ operator.</span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a>:::{#def-halfspace}</span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a>The operator $\mathtt{half-space}$ is such that:</span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>the left input is an S-type set $S$;</span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>the right input is a hyperplane $H$;</span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>the output is the half-spaces of $H$, such that $S$ lies in those half-spaces. </span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a>:::{#def-append:proposition} </span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a>We define the output of $\mathtt{half-space}(S,H)$ by the following rule:</span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>We find two points, $\theta_1, \theta_2 \in \mathbb R^p$, as:</span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\{</span></span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a>\theta_1 &amp;= &amp;Arg\min s(\theta),<span class="sc">\\</span></span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a>\theta_2&amp; =&amp; \left<span class="sc">\{</span></span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a>Arg\min_{\theta \in S} h(\theta),&amp; &amp;\text{if } \theta_1 \in H^+,<span class="sc">\\</span></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a>Arg\max_{\theta \in S} h(\theta), &amp; &amp; \text{if } \theta_1 \in H^-.<span class="sc">\\</span></span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>We have:</span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-703"><a href="#cb2-703" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-704"><a href="#cb2-704" aria-hidden="true" tabindex="-1"></a>\mathtt{half-space}(S,H) = \left<span class="sc">\{</span></span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>H^+<span class="sc">\}</span>, &amp; &amp;\text{if } \theta_1, \theta_2  \in H^+,<span class="sc">\\</span></span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>H^-<span class="sc">\}</span>, &amp; &amp; \text{if } \theta_1, \theta_2  \in H^-,<span class="sc">\\</span></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a><span class="sc">\{</span>H^+, H^-<span class="sc">\}</span>,&amp; &amp;  \text{otherwise}.<span class="sc">\\</span></span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a>\right.</span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a>:::{#lem-petite_lemma}</span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a>$S_1 \subset H_{12}^-\Leftrightarrow \partial S_1 \subset H_{12}^-$, where $\partial(\cdot)$ denote the frontier operator.</span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a>The proof of @lem-petite_lemma follows from the convexity of $S_1$.</span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a>:::{#lem-lemma:inclusion}</span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a>$S_1 \subset S_2$ (resp.  $S_2 \subset S_1$)  $\Leftrightarrow$  $S_1, S_2 \subset H_{12}^-$ (resp. $S_1, S_2 \subset H_{12}^+$).</span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a>We have the hypothesis $\mathcal H_0:<span class="sc">\{</span> S_1 \subset S_2<span class="sc">\}</span>$, then</span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a>\forall \theta \in  \partial S_1 \quad \left<span class="sc">\{</span></span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a>s_1(\theta) = 0, &amp; &amp;<span class="co">[</span><span class="ot">\text{by Definition 1.1} </span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a>s_2(\theta) \le 0, &amp; &amp; <span class="co">[</span><span class="ot">\text{by } \mathcal{H}_0</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a>\right. </span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a>\quad \Rightarrow \theta \in H_{12}^- \quad \Rightarrow \partial S_1 \subset H_{12}^-.</span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a>Thus, according to @lem-petite_lemma, $S_1 \subset H_{12}^-$.</span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a>We have now the hypothesis $\mathcal H_0: <span class="sc">\{</span>S_1, S_2 \subset H_{12}^-<span class="sc">\}</span>$, then</span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a>\forall \theta \in S_1 \quad \left<span class="sc">\{</span></span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-749"><a href="#cb2-749" aria-hidden="true" tabindex="-1"></a>s_1(\theta) \le 0, &amp; &amp;<span class="co">[</span><span class="ot">\text{by Definition 1.1} </span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-750"><a href="#cb2-750" aria-hidden="true" tabindex="-1"></a>h_{12}(\theta) &lt; 0, &amp; &amp; <span class="co">[</span><span class="ot">\text{by } \mathcal{H}_0, \text{ Definition 1.1}</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a>\right. </span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a>\quad \Rightarrow \theta \in  S_2 \quad \Rightarrow S_1 \subset S_2.</span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-755"><a href="#cb2-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-756"><a href="#cb2-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-757"><a href="#cb2-757" aria-hidden="true" tabindex="-1"></a>Similarly, it is easy to show that  $S_2 \subset S_1\Leftrightarrow S_1, S_2 \subset H_{12}^+$.</span>
<span id="cb2-758"><a href="#cb2-758" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-759"><a href="#cb2-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-760"><a href="#cb2-760" aria-hidden="true" tabindex="-1"></a>:::{#lem-lemma:separation}</span>
<span id="cb2-761"><a href="#cb2-761" aria-hidden="true" tabindex="-1"></a>$S_1\cap S_2 = \emptyset \Leftrightarrow H_{12}$ is a separating hyperplane of $S_1$ and $S_2$.</span>
<span id="cb2-762"><a href="#cb2-762" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-763"><a href="#cb2-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-764"><a href="#cb2-764" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb2-765"><a href="#cb2-765" aria-hidden="true" tabindex="-1"></a>We have the hypothesis $\mathcal{H}_0:\{S_1~\subset~ H_{12}^+,\, S_2~\subset~ H_{12}^-\}$. Thus, $H_{12}$ is a separating hyperplane of $S_1$ and $S_2$ then, according to its definition, $S_1\cap S_2 = \emptyset$.</span>
<span id="cb2-766"><a href="#cb2-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-767"><a href="#cb2-767" aria-hidden="true" tabindex="-1"></a>We have now the hypothesis $\mathcal{H}_0:<span class="sc">\{</span>S_1\cap S_2 = \emptyset<span class="sc">\}</span>$ then</span>
<span id="cb2-768"><a href="#cb2-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-769"><a href="#cb2-769" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-770"><a href="#cb2-770" aria-hidden="true" tabindex="-1"></a>\forall \theta \in S_1 \quad \left<span class="sc">\{</span></span>
<span id="cb2-771"><a href="#cb2-771" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-772"><a href="#cb2-772" aria-hidden="true" tabindex="-1"></a>s_1(\theta) \le 0, &amp; &amp;<span class="co">[</span><span class="ot">\text{by Definition 1.1}</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-773"><a href="#cb2-773" aria-hidden="true" tabindex="-1"></a>s_2(\theta) &gt; 0, &amp; &amp; <span class="co">[</span><span class="ot">\text{by } \mathcal{H}_0, \text{ Definition 1.1}</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-774"><a href="#cb2-774" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-775"><a href="#cb2-775" aria-hidden="true" tabindex="-1"></a>\right. </span>
<span id="cb2-776"><a href="#cb2-776" aria-hidden="true" tabindex="-1"></a>\quad \Rightarrow \theta \in  H_{12}^+.</span>
<span id="cb2-777"><a href="#cb2-777" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-778"><a href="#cb2-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-779"><a href="#cb2-779" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb2-780"><a href="#cb2-780" aria-hidden="true" tabindex="-1"></a>\forall \theta \in S_2 \quad \left<span class="sc">\{</span></span>
<span id="cb2-781"><a href="#cb2-781" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb2-782"><a href="#cb2-782" aria-hidden="true" tabindex="-1"></a>s_1(\theta) &gt; 0, &amp; &amp;<span class="co">[</span><span class="ot">\text{by } \mathcal{H}_0, \text{ Definition 1.1}</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-783"><a href="#cb2-783" aria-hidden="true" tabindex="-1"></a>s_2(\theta) \le 0, &amp; &amp; <span class="co">[</span><span class="ot">\text{by Definition 1.1}</span><span class="co">]</span> <span class="sc">\\</span></span>
<span id="cb2-784"><a href="#cb2-784" aria-hidden="true" tabindex="-1"></a>\end{aligned} </span>
<span id="cb2-785"><a href="#cb2-785" aria-hidden="true" tabindex="-1"></a>\right. </span>
<span id="cb2-786"><a href="#cb2-786" aria-hidden="true" tabindex="-1"></a>\quad \Rightarrow \theta \in  H_{12}^-.</span>
<span id="cb2-787"><a href="#cb2-787" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-788"><a href="#cb2-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-789"><a href="#cb2-789" aria-hidden="true" tabindex="-1"></a>Consequently, $H_{12}$ is  a separating hyperplane of  $S_1$ and $S_2$.</span>
<span id="cb2-790"><a href="#cb2-790" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-791"><a href="#cb2-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-792"><a href="#cb2-792" aria-hidden="true" tabindex="-1"></a>:::{#prp-propositionApp}</span>
<span id="cb2-793"><a href="#cb2-793" aria-hidden="true" tabindex="-1"></a>To detect set inclusion $S_1 \subset S_2$ and emptiness of set intersection $S_1 \cap S_2$, it is necessary:</span>
<span id="cb2-794"><a href="#cb2-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-795"><a href="#cb2-795" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>build the hyperplane $H_{12}$;</span>
<span id="cb2-796"><a href="#cb2-796" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>apply the $\mathtt{half-space}$ operator for couples $(S_1,H_{12})$ and $(S_2,H_{12})$ to know in which half-space(s) $S_1$ and $S_2$ are located;</span>
<span id="cb2-797"><a href="#cb2-797" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>check the conditions in Lemmas <span class="co">[</span><span class="ot">-@lem-lemma:inclusion</span><span class="co">]</span> and <span class="co">[</span><span class="ot">-@lem-lemma:separation</span><span class="co">]</span>.</span>
<span id="cb2-798"><a href="#cb2-798" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-799"><a href="#cb2-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-800"><a href="#cb2-800" aria-hidden="true" tabindex="-1"></a><span class="fu">## Proof of @prp-prop_solution_rect {#sec-AppendixD}</span></span>
<span id="cb2-801"><a href="#cb2-801" aria-hidden="true" tabindex="-1"></a>For the proof of @prp-prop_solution_rect we need the following remark.</span>
<span id="cb2-802"><a href="#cb2-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-803"><a href="#cb2-803" aria-hidden="true" tabindex="-1"></a>:::{.remark}</span>
<span id="cb2-804"><a href="#cb2-804" aria-hidden="true" tabindex="-1"></a>With set $S\in \mathbf{S}$ the maximum and minimum values for each coordinate in $S$ are obtained on the axis going through minimal point $\mathbf{c}$.</span>
<span id="cb2-805"><a href="#cb2-805" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-806"><a href="#cb2-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-807"><a href="#cb2-807" aria-hidden="true" tabindex="-1"></a>:::{.proof}</span>
<span id="cb2-808"><a href="#cb2-808" aria-hidden="true" tabindex="-1"></a>Let $\mathbf{c} = <span class="sc">\{</span>\mathbf{c}^k<span class="sc">\}</span>_{k=1,\dots,p}$ is the minimal point of $S$, defined as in @eq-c. In the intersection case, we consider solving the optimization problem (@eq-inclusionOptim) for the boundaries $\tilde{l}^k$ and $\tilde{r}^k$, removing constraint $l^k \le \theta^k \le r^k$. If $R$ intersects $S$, the optimal solution $\theta^k$ belongs to the boundary of $S$ due to our simple (axis-aligned rectangular) inequality constraints and we get </span>
<span id="cb2-809"><a href="#cb2-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-810"><a href="#cb2-810" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-811"><a href="#cb2-811" aria-hidden="true" tabindex="-1"></a>s^k(\theta^k) = -\sum_{ j\neq k}s^j(\theta^j)+ \Delta\,.</span>
<span id="cb2-812"><a href="#cb2-812" aria-hidden="true" tabindex="-1"></a>$${#eq-KKT}</span>
<span id="cb2-813"><a href="#cb2-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-814"><a href="#cb2-814" aria-hidden="true" tabindex="-1"></a>We are looking for minimum and maximum values in $\theta^k$ for this equation with constraints $l^j\le \theta^j \le r^j$ ($j \ne k$). Using the convexity of $s^k$ and $s^j$, we need to maximize the quantity in the right-hand side. Thus, the solution $\tilde{\theta}^j$ for each $\theta^j$ is the minimal value of $\sum_{j\neq k} s^j(\theta^j)$ under constraint $l^j\le \theta^j \le r^j$ and the result can only be $l^j$, $r^j$ or $\mathbf{c}^j$.</span>
<span id="cb2-815"><a href="#cb2-815" aria-hidden="true" tabindex="-1"></a>This decomposition in smaller problems is made possible thanks to our problem setting with independence. Looking at all coordinates at the same time, the values for $\tilde{\theta}\in \mathbb R^p$ corresponds to the closest point $\mathbf{m} =<span class="sc">\{</span>\mathbf{m}^k<span class="sc">\}</span>_{k=1,\dots,p}$. Having found $\theta^{k_1}$ and $\theta^{k_2}$ using $\tilde{\theta}$ the result in @eq-updateIntersection is obvious considering current boundaries $l^k$ and $r^k$.<span class="sc">\\</span></span>
<span id="cb2-816"><a href="#cb2-816" aria-hidden="true" tabindex="-1"></a>In exclusion case, we remove from $R$ the biggest possible rectangle included into $S \cap <span class="sc">\{</span>l^j\le \theta^j \le r^j\,,\, j \ne k<span class="sc">\}</span>$, which correspond to minimizing the right hand side of @eq-KKT, that is maximizing $\sum_{j\neq k} s^j(\theta^j)$ under constraint $l^j\le \theta^j \le r^j$ ($j \ne k$). In that case, the values for $\tilde{\theta}$ correspond to the greatest value returned by  $\sum_{j\neq k} s^j(\theta^j)$ on interval boundaries. With convex functions $s^j$, it corresponds to the farthest point $\mathbf{M} = <span class="sc">\{</span>\mathbf{M}^k<span class="sc">\}</span>_{k=1,\dots, p}$.</span>
<span id="cb2-817"><a href="#cb2-817" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-818"><a href="#cb2-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-819"><a href="#cb2-819" aria-hidden="true" tabindex="-1"></a><span class="fu">## Optimization Strategies for GeomFPOP (R-type) {#sec-AppendixE}</span></span>
<span id="cb2-820"><a href="#cb2-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-821"><a href="#cb2-821" aria-hidden="true" tabindex="-1"></a>In  GeomFPOP(R-type) at each iteration, we need to consider all past and future spheres of change $i$. As it was said in @sec-study, in practice it is often sufficient to consider just a few of them to get an empty set. Thus, we propose to limit the number of operations $\cap_R$ no more than two: </span>
<span id="cb2-822"><a href="#cb2-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-823"><a href="#cb2-823" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathtt{last.}$   At time $t$ we update hyperrectangle by only one operation, this is an intersection with the last S-type set $S^i_t$ from $\mathcal{F}^i(t)$.</span>
<span id="cb2-824"><a href="#cb2-824" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathtt{random.}$ At time $t$ we update the hyperrectangle by only two operations. First, this is an intersection with the last S-type set $S^i_t$ from $\mathcal{F}^i(t)$, and second, this is an intersection with other random  S-type set from $\mathcal{F}^i(t)$.</span>
<span id="cb2-825"><a href="#cb2-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-826"><a href="#cb2-826" aria-hidden="true" tabindex="-1"></a>The number of operations $\setminus_R$ we limit no more than one:</span>
<span id="cb2-827"><a href="#cb2-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-828"><a href="#cb2-828" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathtt{empty.}$ At time $t$ we do not perform $\setminus_R$ operations.</span>
<span id="cb2-829"><a href="#cb2-829" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathtt{random.}$ At time $t$ we update hyperrectangle by only one operation: exclusion with a random  S-type set from $\mathcal{P}^i$.</span>
<span id="cb2-830"><a href="#cb2-830" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-831"><a href="#cb2-831" aria-hidden="true" tabindex="-1"></a>According to these notations, the approach presented in the original GeomFPOP (R-type) has the form $(\mathtt{all / all})$. We show the impact of introduced limits on the number of change point candidates retained over time and evaluate their run times. The results are presented in Figures <span class="co">[</span><span class="ot">-@fig-StrategiesAll</span><span class="co">]</span> and <span class="co">[</span><span class="ot">-@fig-RplotTCoptStrAll</span><span class="co">]</span>.</span>
<span id="cb2-832"><a href="#cb2-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-833"><a href="#cb2-833" aria-hidden="true" tabindex="-1"></a>Even though the $\mathtt{(random/random)}$ approach reduces the quality of pruning in dimensions $p=2,3$ and $4$, it</span>
<span id="cb2-834"><a href="#cb2-834" aria-hidden="true" tabindex="-1"></a> gives a significant gain in the run time compared to the original GeomFPOP (R-type)  and is at least comparable to the $\mathtt{(last/random)}$ approach.</span>
<span id="cb2-835"><a href="#cb2-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-836"><a href="#cb2-836" aria-hidden="true" tabindex="-1"></a>:::{#fig-StrategiesAll}  </span>
<span id="cb2-837"><a href="#cb2-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-838"><a href="#cb2-838" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 10 Optimization Number of candidates.png)</span>{width=80%}</span>
<span id="cb2-839"><a href="#cb2-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-840"><a href="#cb2-840" aria-hidden="true" tabindex="-1"></a>Ratio number of candidate change point over time by different optimization approaches of GeomFPOP (R-type) in dimension $p = 2,3$ and $4$. Averaged over $100$ data sets without changes with $10^4$ data points.</span>
<span id="cb2-841"><a href="#cb2-841" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-842"><a href="#cb2-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-843"><a href="#cb2-843" aria-hidden="true" tabindex="-1"></a>:::{#fig-RplotTCoptStrAll}  </span>
<span id="cb2-844"><a href="#cb2-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-845"><a href="#cb2-845" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/Figure 11 Optimization Time complexity.png)</span>{width=80%}</span>
<span id="cb2-846"><a href="#cb2-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-847"><a href="#cb2-847" aria-hidden="true" tabindex="-1"></a>Run time of different optimization approaches of GeomFPOP (R-type) using multivariate time series without change points. The maximum run time of the algorithms is 3 minutes. Averaged over $100$ data sets.</span>
<span id="cb2-848"><a href="#cb2-848" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-849"><a href="#cb2-849" aria-hidden="true" tabindex="-1"></a><span class="fu"># References {.unnumbered}</span></span>
<span id="cb2-850"><a href="#cb2-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-851"><a href="#cb2-851" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb2-852"><a href="#cb2-852" aria-hidden="true" tabindex="-1"></a>::: </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
for (const element of document.getElementsByClassName("pseudocode")){
    pseudocode.renderElement(element);
}
</script>
<script>
for (const element of document.getElementsByClassName("pseudocode")){
    pseudocode.renderElement(element);
}
</script>
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js" type="text/javascript"></script>
<script type="text/javascript">
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>



</body></html>